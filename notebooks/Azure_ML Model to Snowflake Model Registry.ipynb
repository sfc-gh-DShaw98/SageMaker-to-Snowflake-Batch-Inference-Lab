{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65317f4",
   "metadata": {},
   "source": [
    "# Mortgage Lending Demo in Azure ML\n",
    "This notebook trains a simple XGBoost classifier on your mortgage lending dataset, logs metrics and runs in Azure ML, and registers the resulting model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac60d51",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Upload `MORTGAGE_LENDING_DEMO_DATA.csv` into the **default datastore** of your Azure ML workspace (via Studio ▶ Data ▶ Datastores ▶ Browse ▶ Upload).\n",
    "- Select the **Python 3.10 – AzureML** kernel on your Compute Instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbd8481",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install the v1 SDK into this kernel\n",
    "!pip install azureml-core --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49e9409",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from azureml.core import Workspace, Experiment, Dataset, Model, Run\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9912eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to workspace and load data (replace with your actual values)\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# 🔧 Replace the strings below with your own values\n",
    "subscription_id  = \"<YOUR_SUBSCRIPTION_ID>\"\n",
    "resource_group   = \"<YOUR_RESOURCE_GROUP>\"\n",
    "workspace_name   = \"<YOUR_WORKSPACE_NAME>\"\n",
    "\n",
    "ws = Workspace.get(\n",
    "    name             = workspace_name,\n",
    "    subscription_id  = subscription_id,\n",
    "    resource_group   = resource_group\n",
    ")\n",
    "print(f\"Workspace: {ws.name} (RG: {ws.resource_group})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0889d23",
   "metadata": {},
   "source": [
    "## Step 1: Read Model Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07562a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "ds = Dataset.get_by_name(ws, name=\"collegeaimlops\")\n",
    "df = ds.to_pandas_dataframe()\n",
    "print(f\"Loaded {len(df)} rows from Data asset\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1be232",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Model Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a354cdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows missing target or any required feature\n",
    "required_cols = [\n",
    "    \"MORTGAGERESPONSE\",        # <-- This is your target variable\n",
    "    \"APPLICANT_INCOME_000S\",\n",
    "    \"LOAN_AMOUNT_000S\",\n",
    "    \"LOAN_TYPE_NAME\",          # <-- This is a categorical column\n",
    "    \"LOAN_PURPOSE_NAME\",       # <-- This is a categorical column\n",
    "    \"COUNTY_NAME\",             # <-- This is a categorical column\n",
    "]\n",
    "df_clean = df.dropna(subset=required_cols)\n",
    "\n",
    "# One-hot encode the categoricals (drop_first to avoid dummy trap)\n",
    "categorical_cols = [\"LOAN_TYPE_NAME\", \"LOAN_PURPOSE_NAME\", \"COUNTY_NAME\"]\n",
    "df_encoded = pd.get_dummies(\n",
    "    df_clean,\n",
    "    columns=categorical_cols,\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# Split out X & y, keeping only numeric columns\n",
    "X = df_encoded.drop(columns=[\"MORTGAGERESPONSE\"])\n",
    "X = X.select_dtypes(include=[\"number\"]).copy()\n",
    "\n",
    "# Drop any rows that still have NaNs (just in case)\n",
    "X = X.dropna()\n",
    "\n",
    "y = df_encoded.loc[X.index, \"MORTGAGERESPONSE\"] # <-- Match this to your target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe12e4a",
   "metadata": {},
   "source": [
    "## Step 3: Split into Train, Validation, and Holdout Sets (Split 80/20 then 75/25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b49da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: 80% temp (train + val), 20% holdout (0.2 is your first test_size)\n",
    "X_temp, X_holdout, y_temp, y_holdout = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "# Second split: 75% train, 25% val (0.25 is your second test_size)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Holdout: {len(X_holdout)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b90f6e",
   "metadata": {},
   "source": [
    "## Step 4: Train & Log XGBoost Model using 'logloss' as the evaluation metric in Azure ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b894d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "experiment = Experiment(workspace=ws, name=\"mortgage-xgb-demo\")\n",
    "run = experiment.start_logging()\n",
    "\n",
    "model = XGBClassifier(eval_metric=\"logloss\", use_label_encoder=False)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 1) Get validation predictions\n",
    "val_preds = model.predict(X_val)\n",
    "\n",
    "# 2) Compute metrics\n",
    "val_acc   = accuracy_score(y_val, val_preds)\n",
    "precision = precision_score(y_val, val_preds, average=\"macro\")\n",
    "recall    = recall_score(y_val,    val_preds, average=\"macro\")\n",
    "f1        = f1_score(y_val,        val_preds, average=\"macro\")\n",
    "\n",
    "# 3) Log everything before closing the run\n",
    "run.log(\"validation_accuracy\", val_acc)\n",
    "run.log(\"precision_macro\",     precision)\n",
    "run.log(\"recall_macro\",        recall)\n",
    "run.log(\"f1_macro\",            f1)\n",
    "\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Precision (macro):   {precision:.4f}\")\n",
    "print(f\"Recall (macro):      {recall:.4f}\")\n",
    "print(f\"F1 (macro):          {f1:.4f}\")\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42196b72",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate Model Performance on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e9046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate Model Performance on Validation Set (purely for display)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# reuse val_preds from above\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, val_preds))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, val_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b40c6ba",
   "metadata": {},
   "source": [
    "## Step 6: Allow Azure ML to Access Snowflake\n",
    "In order for your Azure ML instance to connect to Snowflake, you must **add the current public IP address** of this notebook to your Snowflake network policy.\n",
    "\n",
    "#### Step 6A: Get the Public IP of this Azure ML Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9015b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl ifconfig.me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec031378",
   "metadata": {},
   "source": [
    "#### Step 6B: Add the IP to Your Snowflake Network Policy\n",
    "\n",
    "Copy the Step 7A output (ex: `52.183.42.53`) and update your Snowflake network policy by running the following SQL in Snowsight (you must have `ACCOUNTADMIN` privileges):\n",
    "\n",
    "##### Option 1: Create a new network policy (recommended if not already created)\n",
    "```sql\n",
    "-- Create it once\n",
    "CREATE OR REPLACE NETWORK POLICY ALLOW_SAGEMAKER\n",
    "  ALLOWED_IP_LIST = ('<YOUR_SAGEMAKER_IP>')\n",
    "  COMMENT = 'Restrict access to SageMaker IPs for MLOps HOL';\n",
    "-- Assign to service user only\n",
    "ALTER USER mlops_user SET NETWORK_POLICY = ALLOW_SAGEMAKER;\n",
    "-- Verify assignment\n",
    "DESC USER mlops_user;\n",
    "```\n",
    "##### Option 2: Append to an existing IP allowlist (preserve existing IPs)\n",
    "```sql\n",
    "-- If you already have other IPs in the list, you can append your SageMaker IP like this:\n",
    "ALTER NETWORK POLICY ALLOW_SAGEMAKER SET ALLOWED_IP_LIST = (\n",
    "  'existing.ip.1',\n",
    "  'existing.ip.2',\n",
    "  '52.183.42.53'\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b9b3f2",
   "metadata": {},
   "source": [
    "## Step 7: Connect to Snowflake from Azure ML using a .toml File\n",
    "This notebook uses a local .toml file to securely store your Snowflake connection parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcf994c",
   "metadata": {},
   "source": [
    "#### Step 7A: Create a connections.toml file\n",
    "Create a connections.toml file locally with the following structure:\n",
    "```\n",
    "[connections.snowflake_conn]\n",
    "account = \"your_account\"\n",
    "user = \"mlops_user\"\n",
    "role = \"aicollege\"\n",
    "warehouse = \"aicollege\"\n",
    "database = \"aicollege\"\n",
    "schema = \"public\"\n",
    "authenticator = \"snowflake_jwt\"\n",
    "```\n",
    "💡 Replace the values with your actual Snowflake connection details.\n",
    "\n",
    "📝 Reminder: After updating the connections.toml file with your Snowflake account details, make sure to save the file before running the next step.\n",
    "Otherwise, your notebook won’t be able to read the correct connection settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359cbae0",
   "metadata": {},
   "source": [
    "#### Step 7B: Upload the `.toml` and `.pem` Files to Azure ML\n",
    "\n",
    "In Studio, go to Notebooks → click the Upload button in the file-browser pane.\n",
    "\n",
    "Upload your connections.toml and your rsa_private_key.pem into the root directory (or a subfolder).\n",
    "\n",
    "#### Step 7C: Validate Snowpark access with connections.toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4df8411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "# 1) Load your connection settings\n",
    "conn_cfg = toml.load(\"connections.toml\")[\"connections\"][\"Snowpark_MLOps_HOL\"]\n",
    "\n",
    "# 2) Read your PEM and convert to DER bytes\n",
    "with open(\"rsa_private_key.pem\", \"rb\") as f:\n",
    "    pem_data = f.read()\n",
    "\n",
    "private_key = serialization.load_pem_private_key(\n",
    "    pem_data,\n",
    "    password=None,\n",
    "    backend=default_backend()\n",
    ")\n",
    "\n",
    "der_bytes = private_key.private_bytes(\n",
    "    encoding=serialization.Encoding.DER,\n",
    "    format=serialization.PrivateFormat.PKCS8,\n",
    "    encryption_algorithm=serialization.NoEncryption()\n",
    ")\n",
    "\n",
    "# 3) Build your Snowpark Session using DER‐encoded key\n",
    "session = Session.builder.configs({\n",
    "    \"account\":       conn_cfg[\"account\"],\n",
    "    \"user\":          conn_cfg[\"user\"],\n",
    "    \"private_key\":   der_bytes,\n",
    "    \"role\":          conn_cfg[\"role\"],\n",
    "    \"warehouse\":     conn_cfg[\"warehouse\"],\n",
    "    \"database\":      conn_cfg[\"database\"],\n",
    "    \"schema\":        conn_cfg[\"schema\"],\n",
    "    \"authenticator\": conn_cfg[\"authenticator\"],\n",
    "}).create()\n",
    "\n",
    "# 4) Test the connection\n",
    "session.sql(\"SELECT current_user(), current_warehouse(), current_database(), current_schema()\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f09e233",
   "metadata": {},
   "source": [
    "## Step 8: Register Your Azure ML Model in Snowflake Model Registry\n",
    "> **Note:** Unlike AWS SageMaker’s built‐in estimator—which automatically serializes your trained model for you—Azure ML requires you to explicitly save (e.g., via `joblib.dump`) your model artifact before registration.\n",
    "\n",
    "1. **Ensure your Snowflake session is initialized**  \n",
    "   You should already have a working `snowflake.snowpark.Session` in your notebook.\n",
    "\n",
    "2. **Locate your model artifact**  \n",
    "   If you followed the earlier steps, your XGBoost model was saved at:\n",
    "   ```python\n",
    "   model_path = \"outputs/mortgage_xgb_model.pkl\"\n",
    "   ```\n",
    "3. **Call `log_model()`**\n",
    "   Import and invoke the Snowflake ML API to register the model:\n",
    "   ```python \n",
    "   from snowflake.ml.model import log_model\n",
    "\n",
    "   log_model(\n",
    "   model_export_path=model_path,\n",
    "   model_name=\"mortgage_xgb_model\",\n",
    "   model_type=\"xgboost\",\n",
    "   registration_database=\"YOUR_DB\",\n",
    "   registration_schema=\"YOUR_SCHEMA\",\n",
    "   registration_warehouse=\"YOUR_WAREHOUSE\",\n",
    "   replace=True        # overwrite existing version if present\n",
    "   )\n",
    "   ```\n",
    "4. **What happens under the hood?**\n",
    "   - **Manual serialization** of your model file into DER/Pickle.\n",
    "   - **Automatic versioning** and **metadata capture** (framework, tags, metrics).\n",
    "   - **No extra staging**—the API handles uploading for you.\n",
    "5. Why use Snowflake’s Model Registry?\n",
    "   - **Unified storage** beside your data.\n",
    "   - **RBAC & governance** at the schema level.\n",
    "   - **SQL + Python inference** on any table with compatible features.\n",
    "   - **Lineage & observability** for tracking performance and drift.\n",
    "\n",
    "💡 **Pro Tip:** To pull an Azure ML–registered model first, you can use:\n",
    "```python\n",
    "from azureml.core.model import Model\n",
    "model_path = Model(ws, \"mortgage-xgb-model\").download(target_dir=\".\")\n",
    "```\n",
    "then pass that `model_path` into `log_model()` as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f4288c",
   "metadata": {},
   "source": [
    "#### Step 8A: Create Small Input Data for Snowflake Model Registry\n",
    "It helps Snowflake infer the input schema so it knows how to call the model later (during inference, for example). \n",
    "\n",
    "Using a small sample is a best practice to keep things fast and lightweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b6a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a tiny float32 sample of the features you actually trained on\n",
    "sample_input_data = (\n",
    "    X_train\n",
    "    .astype(\"float32\")\n",
    "    .sample(5, random_state=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ff3af",
   "metadata": {},
   "source": [
    "#### Step 8B: Log the trained Azure ML model to Snowflake Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8441005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml.model import type_hints\n",
    "\n",
    "# Grab the metrics you logged in Azure ML\n",
    "metrics = run.get_metrics()\n",
    "\n",
    "# Initialize the Registry pointing at your target database & schema\n",
    "reg = Registry(\n",
    "    session       = session,\n",
    "    database_name = conn_cfg[\"database\"],\n",
    "    schema_name   = conn_cfg[\"schema\"]\n",
    ")\n",
    "\n",
    "# Register the model object you trained in Azure ML\n",
    "mv = reg.log_model(\n",
    "    model_name        = \"azureml_xgb_model\",                        # provide required MLOPs HOL model name\n",
    "    version_name      = \"v1\",                                       # provide required version name\n",
    "    model             = model,                                      # your XGBClassifier instance\n",
    "    sample_input_data = sample_input_data,                          # small pandas df for signature\n",
    "    metrics           = metrics,                                    # dict from run.get_metrics()\n",
    "    conda_dependencies= ['xgboost'],                                # your model’s runtime deps\n",
    "    comment           = \"XGBoost classifier trained in Azure ML\",   # optional description\n",
    "    task              = type_hints.Task.TABULAR_BINARY_CLASSIFICATION,\n",
    "    options           = {'relax_version': False}                    # suppress reproducibility warning\n",
    ")\n",
    "\n",
    "print(f\"✅ Model registered as {mv.model_name} version {mv.version_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d540276f",
   "metadata": {},
   "source": [
    "#### Step 8C: Create and Apply Tags for Model Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f1bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or replace your governance tags in Snowflake\n",
    "session.sql(\"CREATE OR REPLACE TAG MODEL_STAGE_TAG\").collect()\n",
    "session.sql(\"CREATE OR REPLACE TAG MODEL_PURPOSE_TAG\").collect()\n",
    "session.sql(\"CREATE OR REPLACE TAG SOURCE\").collect()\n",
    "session.sql(\"CREATE OR REPLACE TAG PROJECT\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec63179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tags and model-level metadata\n",
    "m = reg.get_model(\"azureml_xgb_model\")   # same name you used when registering\n",
    "\n",
    "# Add model-level description\n",
    "m.comment = \"XGBoost classifier trained in Azure ML to predict mortgage approval.\"\n",
    "\n",
    "# Attach your predefined tags for governance\n",
    "m.set_tag(\"MODEL_STAGE_TAG\",   \"PROD\")                               # deployment stage\n",
    "m.set_tag(\"MODEL_PURPOSE_TAG\", \"Mortgage Response Classification\")   # business context\n",
    "m.set_tag(\"SOURCE\",            \"Azure ML\")                           # origin\n",
    "m.set_tag(\"PROJECT\",           \"College of AI - MLOps HOL\")          # for traceability\n",
    "\n",
    "# View the tags you’ve just set\n",
    "m.show_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6c8646",
   "metadata": {},
   "source": [
    "## Step 9: Preprocess Inference Data for Batch Scoring\n",
    "Before we run batch inference, we need to make sure the incoming Snowflake data matches the **feature format the model was trained on**.\n",
    "\n",
    "Our Azure Machine Learning model expects:\n",
    "- All numeric features\n",
    "- One-hot encoded columns for `LOAN_TYPE_NAME`, `LOAN_PURPOSE_NAME`, and `COUNTY_NAME`  \n",
    "- The same column names (and order) as the training data\n",
    "\n",
    "In this step, we:\n",
    "1. Load `InferenceMortgageData` from Snowflake  \n",
    "2. Apply one-hot encoding with Pandas  \n",
    "3. Fill in any missing dummy columns with 0  \n",
    "4. Reorder columns to match `X_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33375ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull your inference table into a Pandas DataFrame\n",
    "inference_df = (\n",
    "    session\n",
    "    .table(\"INFERENCEMORTGAGEDATA\")     # or session.sql(\"SELECT * FROM INFERENCEMORTGAGEDATA\")\n",
    "    .filter(\"WEEK = 1\")                 # only get data associated with WEEK = 1\n",
    "    .to_pandas()                        # use .to_pandas() in Snowpark\n",
    ")\n",
    "\n",
    "# 2) One-hot encode the same categoricals\n",
    "inference_encoded = pd.get_dummies(\n",
    "    inference_df,\n",
    "    columns=categorical_cols,  # reuse the variable you defined in Step 2\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# 3) Add any missing dummy columns (that X_train had) and fill with 0\n",
    "for col in X_train.columns:\n",
    "    if col not in inference_encoded:\n",
    "        inference_encoded[col] = 0\n",
    "\n",
    "# 4) Reorder to match the training feature order\n",
    "inference_features = inference_encoded[X_train.columns]\n",
    "\n",
    "# Now inference_features is ready to feed into your model:\n",
    "preds = mv.run(\n",
    "    inference_features,\n",
    "    function_name=\"predict\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c46ae7",
   "metadata": {},
   "source": [
    "## Step 10: Batch-scoring Examples\n",
    "\n",
    "In real projects you’ll pick the approach that best matches your scale, latency, and operational constraints. We illustrate three patterns:\n",
    "\n",
    "1. **Step 10A: Quick Demo (Week 1 only)**  \n",
    "   A tiny slice of your data scored via Pandas → Snowflake.  \n",
    "   — ✅ Instant feedback  \n",
    "   — ✅ Perfect for interactive exploration  \n",
    "\n",
    "2. **Step 10B: Full-Dataset Scoring (Pandas-first)**  \n",
    "   Pull all weeks into a Pandas DataFrame, preprocess & align features as during training, run inference with your Snowflake–registered model, then write back one big table.  \n",
    "   — ✅ Familiar end-to-end Python  \n",
    "   — ⚠️ Watch notebook memory on large tables  \n",
    "\n",
    "***Appendix: Snowflake-Native Scoring***  \n",
    "Push your one-hot & alignment logic into a VIEW and call `mv.run()` on that Snowpark DataFrame. This removes the Pandas round-trip but invokes Python UDFs under the hood, which can be slower at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b0a39",
   "metadata": {},
   "source": [
    "#### Step 10A: Week 1 Mortgage Application Scoring\n",
    "Mortgage application data is continuously collected and stored in Snowflake. Rather than scoring in real-time, we use **batch inference** to evaluate applications in bulk — a common approach when scoring latency is not critical.\n",
    "\n",
    "In this step you will:\n",
    "1. **Run inference** on Week 1 application data using your registered XGBoost model (specifying the `predict` entry point).  \n",
    "2. **Format and rename the prediction output** for clarity.  \n",
    "3. **Join predictions with the true outcome** (`MORTGAGERESPONSE`) for monitoring.  \n",
    "4. **Save the results** into a unified table (`PREDICTIONS_WITH_GROUND_TRUTH`) for downstream analysis and drift detection.\n",
    "\n",
    "This unified table enables model observability by tracking how predictions align with actual outcomes over time.\n",
    "\n",
    "> **Reminder:** Your model expects the exact same one-hot-encoded, numeric-only schema you built in Step 11 (`inference_features`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284bbd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run Batch Inference on Week 1 Data and Save Unified Output Table\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# You already have:\n",
    "# - `session`: your Snowpark Session\n",
    "# - `inference_df`: raw WEEK 1 data as pandas\n",
    "# - `inference_features`: one-hot encoded & aligned features as pandas\n",
    "# - `mv`: your ModelVersion object from the Snowflake Registry\n",
    "\n",
    "# Run inference on your preprocessed features\n",
    "predictions      = mv.run(inference_features,   function_name=\"predict\")\n",
    "proba_predictions = mv.run(inference_features,  function_name=\"predict_proba\")\n",
    "\n",
    "# Convert outputs to pandas Series\n",
    "pred_series  = pd.Series(np.squeeze(predictions),      name=\"PREDICTION\")\n",
    "score_series = pd.Series(np.array(proba_predictions)[:,1], name=\"PREDICTED_SCORE\")\n",
    "\n",
    "# Merge predictions back onto the raw input for observability\n",
    "results_df = inference_df.copy()\n",
    "results_df[\"WEEK\"]            = 1\n",
    "results_df[\"PREDICTION\"]      = pred_series\n",
    "results_df[\"PREDICTED_SCORE\"] = score_series\n",
    "\n",
    "# Write the unified results back into Snowflake\n",
    "session.write_pandas(\n",
    "    results_df,\n",
    "    table_name=\"PREDICTIONS_WITH_GROUND_TRUTH\",\n",
    "    auto_create_table=True,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Pull the Snowflake table into pandas\n",
    "pdf = (\n",
    "    session\n",
    "    .table(\"PREDICTIONS_WITH_GROUND_TRUTH\")\n",
    "    .to_pandas()\n",
    ")\n",
    "display(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bf195c",
   "metadata": {},
   "source": [
    "#### Step 10B: Full-Table Scoring (All Weeks) — Pandas-backed\n",
    "\n",
    "Now that you’ve validated scoring on a tiny slice, let’s pull the **entire** mortgage application table into **Pandas** and score it in one shot:\n",
    "\n",
    "- **Load all weeks** into a Pandas DataFrame using `session.table(\"INFERENCEMORTGAGEDATA\").to_pandas()`.  \n",
    "- **One-hot encode & align** the same categorical columns in Pandas so your DataFrame matches `X_train` exactly.  \n",
    "- **Run inference** via `mv.run(..., function_name=\"predict\")` and `mv.run(..., function_name=\"predict_proba\")` on the Pandas DataFrame.  \n",
    "- **Merge predictions** back onto your raw data and write the full results into Snowflake with `session.write_pandas(..., overwrite=True)`.\n",
    "\n",
    "> **Tip:** This end-to-end Pandas approach feels familiar and scales well up to hundreds of thousands (even low millions) of rows—just keep an eye on your notebook’s memory if the table grows very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ff4e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "# 1) Load the _entire_ inference dataset into pandas\n",
    "raw_all = session.table(\"INFERENCEMORTGAGEDATA\").to_pandas()\n",
    "\n",
    "# 2) One-hot encode the same categoricals (drop_first to match training)\n",
    "encoded_all = pd.get_dummies(\n",
    "    raw_all,\n",
    "    columns=categorical_cols,\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# 3) Add any columns X_train had that this week didn’t, then reorder\n",
    "for c in X_train.columns:\n",
    "    if c not in encoded_all.columns:\n",
    "        encoded_all[c] = 0\n",
    "encoded_all = encoded_all[X_train.columns]\n",
    "\n",
    "# 4) Load your model version from the registry\n",
    "reg = Registry(session=session,\n",
    "               database_name=conn_cfg[\"database\"],\n",
    "               schema_name=conn_cfg[\"schema\"])\n",
    "model = reg.get_model(\"azureml_xgb_model\")    # <-- match your model_name\n",
    "\n",
    "# 5) Run inference\n",
    "preds  = mv.run(encoded_all,     function_name=\"predict\")\n",
    "probas = mv.run(encoded_all,     function_name=\"predict_proba\")\n",
    "\n",
    "# 6) Attach back into the raw dataframe\n",
    "raw_all[\"PREDICTED_RESPONSE\"] = np.squeeze(preds)\n",
    "raw_all[\"PREDICTED_SCORE\"]    = np.array(probas)[:,1]\n",
    "\n",
    "# 7) Push it all back into Snowflake\n",
    "results_sp = session.create_dataframe(raw_all)\n",
    "results_sp.write.mode(\"overwrite\") \\\n",
    "          .save_as_table(\"ALL_PREDICTIONS_WITH_GROUND_TRUTH\")\n",
    "\n",
    "# 8) Quick sanity-check\n",
    "pdf = results_sp.limit(10).to_pandas()\n",
    "display(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c937e5a",
   "metadata": {},
   "source": [
    "## Scaling Inference with Snowflake ML Jobs and SPCS\n",
    "\n",
    "Once you've validated your **Pandas-backed** batch scoring approach, production workloads typically require more scalable execution environments. Snowflake provides two powerful options:\n",
    "\n",
    "### 📋 **Snowflake ML Jobs**\n",
    "Schedule and orchestrate your end-to-end ML pipelines using Snowflake's managed **Container Runtime**.\n",
    "\n",
    "**Key Benefits:**\n",
    "-✅ **IDE Integration**: Dispatch jobs from VS Code, PyCharm, or Azure ML\n",
    "- ✅ **Serverless Execution**: Auto-scaling with no infrastructure management  \n",
    "- ✅ **Cost-Effective**: Pay only for compute time used\n",
    "\n",
    "**Best For:** Scheduled batch inference, model retraining pipelines, feature engineering workflows\n",
    "\n",
    "---\n",
    "\n",
    "### 🐳 **Snowpark Container Services (SPCS)**\n",
    "Deploy your model as containerized services or jobs on dedicated compute pools with full flexibility.\n",
    "\n",
    "**Key Benefits:**\n",
    "- ✅ **No Package Restrictions**: Use any Python packages from PyPI\n",
    "- ✅ **GPU Support**: Scale to large models with distributed GPU clusters\n",
    "- ✅ **Service Endpoints**: Deploy always-on inference APIs\n",
    "\n",
    "**Best For:** Real-time inference endpoints, large-scale bulk scoring, GPU-accelerated workloads\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 **Choosing the Right Approach**\n",
    "\n",
    "| Scenario | Recommended Solution | Key Consideration |\n",
    "|----------|---------------------|-------------------|\n",
    "| **Scheduled batch inference** | ML Jobs | Finite-duration, cost-effective |\n",
    "| **Always-on inference API** | SPCS Services | Set `MIN_INSTANCES = MAX_INSTANCES` |\n",
    "| **Large bulk scoring** | SPCS Jobs | Handle enterprise-scale datasets |\n",
    "\n",
    "### 📚 **Documentation & Next Steps**\n",
    "\n",
    "- **[ML Jobs Guide](https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-mlops)** - Automated ML pipelines\n",
    "- **[SPCS Overview](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/overview)** - Container services\n",
    "- **[Model Serving](https://docs.snowflake.com/en/developer-guide/snowpark-ml/model-registry/model-serving-spcs)** - Deploy models to SPCS\n",
    "\n",
    "**💡 Pro Tip:** Start with ML Jobs for your weekly batch scoring, then consider SPCS if you need real-time inference or GPU acceleration.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

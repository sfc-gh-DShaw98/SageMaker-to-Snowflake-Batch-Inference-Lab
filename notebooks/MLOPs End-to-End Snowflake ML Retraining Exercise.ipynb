{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "kqvucnm5kvl43iuira3y",
   "authorId": "154296475017",
   "authorName": "DSHAW_SFC",
   "authorEmail": "diana.shaw@snowflake.com",
   "sessionId": "80034bbc-c0ca-49d0-9ad9-c185750a180a",
   "lastEditTime": 1752777162181
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "End2EndSnowflakeMLFlow"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "\n",
    "# Define image in a stage and read the file\n",
    "image=session.file.get_stream('@aicollege.public.setup/SnowflakeML.jpg' , decompress=False).read() \n",
    "\n",
    "# Display the image\n",
    "st.image(image, width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "collapsed": false,
    "name": "NotebookOverview"
   },
   "source": [
    "### ❄️ End-to-End ML Retraining with Snowflake and Open-Source Models ❄️\n",
    "\n",
    "This section walks through the full retraining pipeline using **Snowflake-native data**, **open-source ML libraries**, and the **Snowflake Model Registry** to version, monitor, and explain updated model behavior.\n",
    "\n",
    "Before running this notebook, make sure the enriched dataset has been loaded into the `NEWTRAININGDATA` table. This table combines historical and recent mortgage data to improve generalization and reduce drift.\n",
    "\n",
    "In this retraining pipeline, we will:\n",
    "\n",
    "- Load and explore the enriched data from the `NEWTRAININGDATA` table  \n",
    "- Preprocess inputs and engineer features  \n",
    "- Register features into the **Snowflake Feature Store**\n",
    "- Retrain a model using **open-source ML frameworks** like `xgboost` or `scikit-learn`\n",
    "- Register the updated model as **Version 2** in the **Snowflake Model Registry**\n",
    "- Set up **ML Monitors** to compare V1 vs. V2 on performance drift\n",
    "- Use **Snowflake ML Explainability** (`EXPLAIN`) to generate SHAP-style insights\n",
    "- Promote the best-performing model version to production using **aliases or version control**\n",
    "- Run **dynamic batch scoring** in Python **using the production alias**\n",
    "\n",
    "> ✅ This notebook assumes your original model (Version 1) is already registered as `COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL` in the Snowflake Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "id": "d8d00a29-4e7d-4080-9d30-f0f4308089b9",
   "metadata": {
    "language": "sql",
    "name": "NotebookReadinessCheck"
   },
   "outputs": [],
   "source": "-- Validate query COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL version V1 is registered in Snowflake Model Registry\nSHOW VERSIONS IN MODEL COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ac6c0562-465c-4009-9d48-4dc7c2c5f34b",
   "metadata": {
    "collapsed": false,
    "name": "LoadRetrainingData"
   },
   "source": [
    "### 📥 Load and Inspect the Enriched Training Dataset\n",
    "\n",
    "In this step, we load the `NEWTRAININGDATA` table, which contains a curated dataset designed to support retraining on more current and representative patterns. It includes:\n",
    "\n",
    "- Original training data capturing historical trends\n",
    "- Recent labeled examples from production (e.g., batch inference results with ground truth)\n",
    "- Optionally, synthetic records to match the distribution of current input patterns\n",
    "\n",
    "Rather than using a simple `SELECT * ... LIMIT`, we’ll preview the data using a query that returns a **sample of rows per `LOAN_PURPOSE_NAME`**. This approach gives a more balanced snapshot of the dataset across different loan purposes, which helps validate readiness for downstream preprocessing and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "sql",
    "name": "ViewRetrainingData"
   },
   "outputs": [],
   "source": [
    "SELECT * EXCLUDE (rn)\n",
    "FROM (\n",
    "    SELECT *,\n",
    "           ROW_NUMBER() OVER (PARTITION BY LOAN_TYPE_NAME ORDER BY LOAN_ID) AS rn\n",
    "    FROM AICOLLEGE.PUBLIC.NEWTRAININGDATA\n",
    ")\n",
    "WHERE rn <= 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e728c10b-1e7e-43b9-94b5-12128a02779b",
   "metadata": {
    "collapsed": false,
    "name": "Preprocessing"
   },
   "source": [
    "### 🧹 Data Cleaning & Sanitization\n",
    "\n",
    "Before we apply One-Hot Encoding or register features to the Snowflake Feature Store, we create a cleaned and standardized version of the training data called `NEWTRAININGDATA_CLEANED`. This ensures downstream steps operate on consistent, ML-friendly inputs.\n",
    "\n",
    "In this step, we:\n",
    "\n",
    "- **Map known loan-type values**  \n",
    "  Standardize strings for model compatibility:  \n",
    "  - `FSA/RHS-guaranteed` → `FSA_RHS`  \n",
    "  - `FHA-insured` → `FHA`  \n",
    "  - `VA-guaranteed` → `VA`  \n",
    "\n",
    "- **Normalize key categorical fields**  \n",
    "  - `LOAN_PURPOSE_NAME`: Replace hyphens, spaces, and periods with underscores (e.g., `home-improvement` → `home_improvement`)  \n",
    "  - `COUNTY_NAME`: Remove hyphens, underscores, and periods entirely (e.g., `St. Lawrence County` → `St Lawrence County`)  \n",
    "\n",
    "- **Preserve temporal and ID metadata**  \n",
    "  Retain `WEEK_START_DATE`, `WEEK`, `LOAN_ID`, and `TS` fields for observability and feature tracking.\n",
    "\n",
    "- **Ensure correct numeric types**  \n",
    "  Explicitly cast `APPLICANT_INCOME_000S` and `LOAN_AMOUNT_000S` as `FLOAT` to prevent type issues during feature engineering or training.\n",
    "\n",
    "The resulting `NEWTRAININGDATA_CLEANED` table provides a consistent and sanitized foundation for one-hot encoding, feature store registration, and model retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a3e98c-02fb-423b-b716-adf0601f1cf1",
   "metadata": {
    "language": "sql",
    "name": "PreprocessingCode"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE AICOLLEGE.PUBLIC.NEWTRAININGDATA_CLEANED AS\n",
    "SELECT\n",
    "  -- normalize loan type\n",
    "  REPLACE(\n",
    "    REPLACE(\n",
    "      REPLACE(LOAN_TYPE_NAME,\n",
    "        'FSA/RHS-guaranteed', 'FSA_RHS'),\n",
    "        'FHA-insured', 'FHA'),\n",
    "        'VA-guaranteed', 'VA'\n",
    "  ) AS LOAN_TYPE_NAME,\n",
    "\n",
    "  -- normalize loan purpose\n",
    "  REPLACE(\n",
    "    REPLACE(\n",
    "      REPLACE(LOAN_PURPOSE_NAME,\n",
    "        '-', '_'),\n",
    "        ' ', '_'),\n",
    "        '.', '_'\n",
    "  ) AS LOAN_PURPOSE_NAME,\n",
    "\n",
    "  -- normalize county\n",
    "  REPLACE(\n",
    "    REPLACE(\n",
    "      REPLACE(COUNTY_NAME,\n",
    "        '-', ''),   -- remove hyphens\n",
    "        '_', ''),   -- remove underscores\n",
    "        '.', ''     -- remove periods\n",
    "  ) AS COUNTY_NAME,\n",
    "\n",
    "  WEEK_START_DATE,\n",
    "  WEEK,\n",
    "  LOAN_ID,\n",
    "  TS,\n",
    "  APPLICANT_INCOME_000S,\n",
    "  LOAN_AMOUNT_000S,\n",
    "  MORTGAGERESPONSE\n",
    "FROM AICOLLEGE.PUBLIC.NEWTRAININGDATA;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79e614a-77e6-410c-853c-435080de6a39",
   "metadata": {
    "collapsed": false,
    "name": "FeatureEngineeringFeatureStore"
   },
   "source": [
    "### 🧪 Feature Engineering & Feature Store Setup\n",
    "\n",
    "In this phase, we define and register a consistent set of engineered features using the cleaned `NEWTRAININGDATA_CLEANED` dataset as input. These features are transformed using **Snowflake ML preprocessing** and stored in the **Feature Store** for reuse across training and scoring workflows.\n",
    "\n",
    "The **Snowflake Feature Store** enables:\n",
    "- ✅ Centralized tracking of feature definitions, lineage, and versions  \n",
    "- ♻️ Reuse of production-ready features across both training and inference pipelines  \n",
    "- 🔍 Transparent, reproducible, and maintainable ML workflows\n",
    "\n",
    "---\n",
    "\n",
    "### What we’ll do:\n",
    "1. Identify categorical features (`LOAN_TYPE_NAME`, `LOAN_PURPOSE_NAME`, `COUNTY_NAME`) and apply **One-Hot Encoding (OHE)** using `OneHotEncoder` from *snowflake.ml*.  \n",
    "2. Keep the original raw columns for **transparency and lineage**.  \n",
    "3. Fill in any missing values to ensure clean inputs.  \n",
    "4. Write the transformed dataset to a persistent table `NEWTRAININGDATA_FINAL`.  \n",
    "5. Optionally register engineered features in the Feature Store for consistent batch inference and monitoring.\n",
    "\n",
    "> 🧱 Separating raw definitions from model-specific transformations gives us:\n",
    "> - 🪄 Reusable and modular feature logic  \n",
    "> - 🔁 Flexibility to experiment with alternative encoding strategies  \n",
    "> - 🧪 Reliable inputs for training, production scoring, and explainability\n",
    "\n",
    "> ⚠️ **NumPy 2.0 Note**:  \n",
    "> Snowflake ML expects the `np.float_` alias, which was removed in NumPy ≥ 2.0. Add this shim before importing `OneHotEncoder`:\n",
    "> ```python\n",
    "> import numpy as np  \n",
    "> if not hasattr(np, \"float_\"):  \n",
    ">     np.float_ = np.float64  \n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3407fc-ac37-483d-b29c-0d815fa3a91c",
   "metadata": {
    "language": "python",
    "name": "FeatureEngineeringCode"
   },
   "outputs": [],
   "source": "# --- Feature Engineering with OneHotEncoder in Snowflake ML ---\nimport numpy as np\nfrom snowflake.snowpark.functions import col\nfrom snowflake.snowpark.types import StringType, IntegerType, FloatType, DoubleType, DecimalType\nfrom snowflake.ml.modeling.preprocessing import OneHotEncoder\n\n# Temporary patch for NumPy >= 2.0\nif not hasattr(np, 'float_'):\n    np.float_ = np.float64\n\n# Load cleaned training data\ndf_retrain = session.table(\"AICOLLEGE.PUBLIC.NEWTRAININGDATA_CLEANED\")\n\n# Identify categorical columns (exclude timestamp and label)\ncategorical_cols = [\n    f.name\n    for f in df_retrain.schema\n    if isinstance(f.datatype, StringType)\n       and f.name not in (\"TS\", \"MORTGAGERESPONSE\")\n]\n\n# Create output column names with _OHE suffix\nencoded_cols = [f\"{c}_OHE\" for c in categorical_cols]\n\n# Instantiate OneHotEncoder\nohe = XXX(        # --> Use Snowflake's OneHotEncoder function\n    input_cols=categorical_cols,\n    output_cols=encoded_cols,\n    drop_input_cols=False\n)\n\n# Apply encoder\ndf_encoded = ohe.fit(df_retrain).transform(df_retrain)\n\n# Identify numeric columns only for safe fillna\nnumeric_cols = [\n    f.name for f in df_encoded.schema\n    if isinstance(f.datatype, (IntegerType, FloatType, DoubleType, DecimalType))\n]\n\n# Apply fillna only to numeric columns\ndf_encoded = df_encoded.fillna({col_name: 0 for col_name in numeric_cols})\n\n# Save encoded data to Snowflake table\ndf_encoded.write.mode(\"overwrite\").save_as_table(\"AICOLLEGE.PUBLIC.NEWTRAININGDATA_FINAL\")  # --> Save as table AICOLLEGE.PUBLIC.NEWTRAININGDATA_FINAL\n\n# Reload as a persisted DataFrame\ndf_encoded_persisted = session.table(\"AICOLLEGE.PUBLIC.NEWTRAININGDATA_FINAL\")"
  },
  {
   "cell_type": "markdown",
   "id": "27dc2167-0003-49e0-92fc-2d9b59c033be",
   "metadata": {
    "collapsed": false,
    "name": "FeatureStore"
   },
   "source": [
    "### 🗄️ Register Loan Features with the Feature Store\n",
    "\n",
    "Now that we’ve packaged our cleaned, one-hot encoded data (`NEWTRAININGDATA_FINAL`),  \n",
    "we’ll register exactly the features our model needs in the **Snowflake Feature Store**.\n",
    "\n",
    "Why this matters:\n",
    "- 🎯 **Single source of truth** - the same features feed training, inference, and monitoring.\n",
    "- 🔍 **Time‑aware joins & lineage** – every feature is versioned and timestamped.\n",
    "- 🔄 **Model‑agnostic** – any future model can reuse the view without new ETL.\n",
    "\n",
    "In this step, we will:\n",
    "1. Define a `LOAN_ENTITY` on the `LOAN_ID` primary key  \n",
    "2. Select only the OHE columns (those containing `\"_OHE_\"`) **plus** the numeric features  \n",
    "   - `APPLICANT_INCOME_000S`  \n",
    "   - `LOAN_AMOUNT_000S`  \n",
    "   - `WEEK`  \n",
    "3. **Exclude** any leakage columns (`MORTGAGERESPONSE`, `TS`, etc.)  \n",
    "4. Register a Feature View (e.g. `LOAN_FEATURES` or `OHE_FEATURE_VIEW`)  \n",
    "   using `WEEK_START_DATE` as the timestamp for time-travel joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5f183-b940-44b9-876f-477059ed84a5",
   "metadata": {
    "language": "python",
    "name": "FeatureStoreCode"
   },
   "outputs": [],
   "source": "# --- Register One-Hot Encoded Features with Snowflake Feature Store \nimport warnings\nfrom snowflake.ml.feature_store import FeatureStore, FeatureView, Entity, CreationMode\nfrom snowflake.snowpark.functions import col\n\n# 1) Read your encoded table\ndf_encoded = session.table(\"AICOLLEGE.PUBLIC.NEWTRAININGDATA_FINAL\")\n\n# 2) Identify raw feature columns\nnumeric_cols = [\"APPLICANT_INCOME_000S\", \"LOAN_AMOUNT_000S\", \"WEEK\"]\nraw_ohe_cols = [c for c in df_encoded.columns if \"_OHE_\" in c]\n\n# 3) Build a rename map to strip quotes & replace spaces/dots with underscores\ndef sanitize(name):\n    return name.strip('\"').replace(\" \", \"_\").replace(\".\", \"_\")\n\nrename_map = {c: sanitize(c) for c in [\"LOAN_ID\", \"WEEK_START_DATE\"] + raw_ohe_cols + numeric_cols}\n\n# 4) Select & rename in one go\ndf_features = df_encoded.select([\n    col(old).alias(new)\n    for old, new in rename_map.items()\n])\n\n# 5) Initialize Feature Store\nfs = XXX(  # --> Use Snowflake's Feature Store (FeatureStore)\n    session=session,\n    database=\"AICOLLEGE\",\n    name=\"PUBLIC\",\n    default_warehouse=\"AICOLLEGE\",\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n)\n\n# 6) Register entity\nloan_entity = XXX(name=\"XXX\", join_keys=[\"LOAN_ID\"])  # --> Save as LOAN_ENTITY as the Feature Store Entity\nfs.register_entity(loan_entity)\n\n# 7) Register Feature View\nfv = XXX(  # --> Use Snowflake's Feature View (FeatureView)\n    name=\"XXX\",  # --> Save as LOAN_FEATURES_OHE Feature View\n    entities=[loan_entity],\n    feature_df=df_features,\n    timestamp_col=\"WEEK_START_DATE\",  # --> Ensure you include your TIMESTAMP column with TIMESTAMP_NTZ format (WEEK_START_DATE)\n    refresh_freq=\"1 day\"  # --> Ensure your Snowflake Feature Store (dynamic table) has a valid refresh frequency value\n)\nfs.register_feature_view(fv, version=\"1\", overwrite=True)\nprint(\"✅ Feature View registered with\", len(rename_map), \"features:\", list(rename_map.values()))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a25584-2d84-4927-841c-2333a2869dbf",
   "metadata": {
    "language": "sql",
    "name": "ViewFeatureView"
   },
   "outputs": [],
   "source": [
    "-- List dynamic tables created by the Feature Store for OHE_FEATURE_VIEW.\n",
    "SHOW TABLES LIKE 'LOAN_FEATUREs_OHE%';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84828c2a-b30d-49bc-bfc3-9c4cceac0850",
   "metadata": {
    "collapsed": false,
    "name": "RetrainModel"
   },
   "source": "### 🔄 Retrain and Compare ML Models (Using OSS Libraries)\n\nNow that our **one-hot encoded features** are saved in the `NEWTRAININGDATA_FINAL` table, we’ll **retrain an updated XGBoost model** using the enriched training dataset.\n\nThis step includes:\n\n- **Loading** the feature-engineered data from Snowflake  \n- Converting it to a **Pandas DataFrame** for modeling  \n- Splitting into **train (80% ≈ 4,240 samples)** and **test (20% ≈ 1,060 samples)** sets  \n- **Retraining & evaluating** a new XGBoost Classifier model\n- Comparing its performance using **test set accuracy**\n\n### ❗ Why use Pandas?\n\nWhile your data lives in Snowflake, OSS libraries like `scikit-learn` and `xgboost` require **in-memory data**. They do **not support Snowpark DataFrames** directly.  \nFor that reason, we convert Snowflake data to a Pandas DataFrame before training models locally in the notebook.\n\nThis approach:\n- ✅ Aligns with Snowflake’s guidance to use OSS libraries for modeling  \n- ⚙️ Enables flexible development and model experimentation  \n- 🔁 Keeps the pipeline compatible with Snowflake Model Registry and Batch Inference later"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e507113a-0c27-48af-8baa-f1063ed7c661",
   "metadata": {
    "language": "python",
    "name": "ModelSetup"
   },
   "outputs": [],
   "source": "# --- Model setup using only 'LOAN_PURPOSE_NAME_OHE' to reduce dimensionality ---\nfrom snowflake.snowpark.functions import col\nfrom snowflake.snowpark.types import FloatType\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# 1. Load OHE features — only keep what you need\ndf_features_all = session.table(\"AICOLLEGE.PUBLIC.LOAN_FEATURES_OHE$1\")\n\n# 2. Filter columns\nohe_cols_to_keep = [c for c in df_features_all.columns if c.startswith(\"LOAN_PURPOSE_NAME_OHE\")]\nselected_cols = [\"LOAN_ID\", \"WEEK_START_DATE\", \"APPLICANT_INCOME_000S\", \"LOAN_AMOUNT_000S\"] + ohe_cols_to_keep\ndf_features = df_features_all.select([col(c) for c in selected_cols])\n\n# 3. Load labels\ndf_labels = session.table(\"AICOLLEGE.PUBLIC.NEWTRAININGDATA_FINAL\") \\\n                   .select(\"LOAN_ID\", \"WEEK_START_DATE\", \"MORTGAGERESPONSE\")\n\n# 4. Join + cast numeric columns in Snowpark\ndf_joined = (\n    df_features.join(df_labels, on=[\"LOAN_ID\", \"WEEK_START_DATE\"])\n    .with_column(\"APPLICANT_INCOME_000S\", col(\"APPLICANT_INCOME_000S\").cast(FloatType()))\n    .with_column(\"LOAN_AMOUNT_000S\", col(\"LOAN_AMOUNT_000S\").cast(FloatType()))\n    .to_pandas()\n)\n\n# 5. Clean column names and drop missing\ndf_joined.columns = df_joined.columns.str.strip('\"').str.replace(\" \", \"_\")\ndf_joined = df_joined.dropna()\n\n# 6. Define X, y and keys\nlabel = \"MORTGAGERESPONSE\"  # --> Which column is the target variable?\nfeature_cols = ohe_cols_to_keep + [\"APPLICANT_INCOME_000S\", \"LOAN_AMOUNT_000S\"]\nX = df_joined[feature_cols]\ny = df_joined[label].astype(int)\nid_vec    = df_joined[\"LOAN_ID\"].astype(int)\ndate_vec  = pd.to_datetime(df_joined[\"WEEK_START_DATE\"])\n\n# 7. Train/test split for all 4 arrays\nX_train, X_test, y_train, y_test, id_train, id_test, date_train, date_test = train_test_split(\n    X, y, id_vec, date_vec,\n    test_size=0.2,\n    random_state=42\n)\n\n# 8. Report\nprint(f\"✅ Train set: {X_train.shape[0]} samples ({X_train.shape[0]/len(df_joined)*100:.1f}%)\")\nprint(f\"✅ Test  set: {X_test.shape[0]} samples ({X_test.shape[0]/len(df_joined)*100:.1f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a78f28-8831-41bc-985d-ac2f96326fa8",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "TrainModel"
   },
   "outputs": [],
   "source": "import pandas as pd\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Let's retrain the model with another XGBoost Classifier model\n# Initialize XGBoost Classifier model\n# objective='binary:logistic' for binary classification (outputs probabilities)\n# eval_metric='logloss' is a common evaluation metric for binary classification\nlogreg_model = XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=42)\n\n# Train the model\nprint(\"\\nTraining XGBoost model...\")\nlogreg_model.fit(X_train, y_train)\nprint(\"Model training complete.\")\n\n# Make predictions on the test set\ny_pred_proba = logreg_model.predict_proba(X_test)[:, 1] # Get probabilities for the positive class\ny_pred = (y_pred_proba > 0.5).astype(int) # Convert probabilities to binary predictions (0 or 1)\n\n# Evaluate the model\nprint(\"\\nModel Evaluation:\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n\n# Display some predicted probabilities\nprint(\"\\nSample Predicted Probabilities (first 10):\")\nprint(y_pred_proba[:10])\nprint(\"\\nSample Actual Labels (first 10):\")\nprint(y_test.head(10).values)"
  },
  {
   "cell_type": "code",
   "id": "b7c5861b-2fd3-457a-a000-e3a3309acbc3",
   "metadata": {
    "language": "python",
    "name": "PersistV2Predictions"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import col, row_number, sql_expr\nfrom snowflake.snowpark.window import Window\n\n# Build the prediction DataFrame as pandas\ndf_raw = pd.DataFrame({\n    \"LOAN_ID\":            id_test.values,\n    \"WEEK_START_DATE\":    date_test.values,\n    \"PREDICTED_RESPONSE\": logreg_model.predict(X_test).astype(int),\n    \"PREDICTED_SCORE\":    logreg_model.predict_proba(X_test)[:, 1],\n    \"MORTGAGERESPONSE\":   y_test.values.astype(int),\n})\n\n# Convert to Snowpark DataFrame without passing schema\ndf_snowpark = session.create_dataframe(df_raw)\n\n# Define a Snowpark window by WEEK_START_DATE with random sort\nwindow_spec = Window.partition_by(\"WEEK_START_DATE\").order_by(sql_expr('RANDOM()'))\n\n# Assign row number for sampling\ndf_with_row_number = df_snowpark.with_column(\"row_num\", row_number().over(window_spec))\n\n# Filter to first 200 rows per WEEK_START_DATE\ndf_sampled = df_with_row_number.filter(col(\"row_num\") <= 200).drop(\"row_num\")\n\n# Persist to Snowflake\ndf_sampled.write.mode(\"overwrite\").save_as_table(\"AICOLLEGE.PUBLIC.V2_RAW_PREDICTIONS\")\n\nprint(\"✅ V2 prediction sets persisted for Model Monitor.\")\ndf_sampled.limit(5).show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af9a0787-e9dd-4150-8982-170c4086ee20",
   "metadata": {
    "language": "sql",
    "name": "CreateV2PredictionsView"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE VIEW AICOLLEGE.PUBLIC.V2_PREDICTIONS AS\nWITH b_deduped AS (\n  SELECT *\n  FROM (\n    SELECT *,\n           ROW_NUMBER() OVER (PARTITION BY LOAN_ID, WEEK_START_DATE ORDER BY TS DESC) AS rn\n    FROM AICOLLEGE.PUBLIC.NEWTRAININGDATA_FINAL\n  )\n  WHERE rn = 1\n)\nSELECT\n  b.WEEK_START_DATE,\n  b.WEEK,\n  b.LOAN_ID,\n  b.TS,\n  b.LOAN_TYPE_NAME,\n  b.LOAN_PURPOSE_NAME,\n  CAST(b.APPLICANT_INCOME_000S AS FLOAT) AS APPLICANT_INCOME_000S,\n  b.LOAN_AMOUNT_000S,\n  b.COUNTY_NAME,\n  r.MORTGAGERESPONSE,\n  r.PREDICTED_RESPONSE,\n  r.PREDICTED_SCORE\nFROM AICOLLEGE.PUBLIC.V2_RAW_PREDICTIONS AS r\nJOIN b_deduped AS b\n  USING (LOAN_ID, WEEK_START_DATE);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ae24910d-b8f1-464a-9981-bf0225a640c4",
   "metadata": {
    "collapsed": false,
    "name": "RegisterModel"
   },
   "source": "### 📦 Register Updated Model (Version 2)\n\nWe retrained our **XGBoost** classifier on the enriched `NEWTRAININGDATA` table and it delivered the best overall performance.  \nWe will now register this XGBoost model in the **Snowflake Model Registry**.\n\nDuring registration we will log:\n- The full model artifact  \n- Version metadata for lifecycle management  \n- Key training metrics (accuracy, F1, confusion matrix, etc.)\n\n> ℹ️ **Note:**  \n> We are **not yet assigning a production alias** (like `production` or `current_best`) — aliasing will happen after observability evaluation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a216bb63-3dea-4ec9-8778-b55aa6fb1634",
   "metadata": {
    "language": "python",
    "name": "RegisterRetrainedModel"
   },
   "outputs": [],
   "source": "# --- ✅ Register OSS Logistic Regression as V2 of Existing Model ---\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.model import type_hints\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\nimport logging\n\nsession  = get_active_session()\nregistry = Registry(session=session)\n\n# Suppress verbose Snowflake connector logs\nlogging.getLogger(\"snowflake.connector\").setLevel(logging.WARNING)\nlogging.getLogger(\"snowflake.ml\").setLevel(logging.WARNING)\nlogging.basicConfig(level=logging.WARNING)\n\n# ── Evaluate the XGBoost model ────────────────────────────────────────────────\ny_pred       = logreg_model.predict(X_test)\naccuracy     = accuracy_score(y_test, y_pred)\nf1           = f1_score(y_test, y_pred)\nreport       = classification_report(y_test, y_pred, output_dict=True)\nconf_matrix  = confusion_matrix(y_test, y_pred).tolist()\n\n# Sample rows passed to Registry for schema inference & explainability\nsample_input_data = X_train.copy().sample(n=5, random_state=42)\n\n# ── Register as Version 2 of your mortgage-model ─────────────────────────────\nmodel_version = registry.log_model(\n    model              = logreg_model,                          # your XGBClassifier object\n    model_name         = \"XXX\",                                 # must match your registered name\n    version_name       = \"XXX\",                                 # new version identifier\n    sample_input_data  = sample_input_data,\n    # XGBoost is the critical dependency; include scikit-learn for metric eval\n    conda_dependencies = [\"xgboost\", \"scikit-learn\"],     \n    comment = \"\"\"Version 2: XGBoost binary-classification model (objective='binary:logistic').\nSupports Snowflake ML Observability & Explainability.\"\"\",\n    metrics = {\n        \"accuracy\"            : accuracy,\n        \"f1_score\"            : f1,\n        \"classification_report\": report,\n        \"confusion_matrix\"    : conf_matrix\n    },\n    task             = type_hints.Task.TABULAR_BINARY_CLASSIFICATION,\n    target_platforms = [\"WAREHOUSE\"],                     # run inside Snowflake warehouse\n    options          = {\n        \"method_options\": {\n            \"predict\": {\"case_sensitive\": True}\n        },\n    }\n)\n\nprint(f\"✅ XGBoost model registered as {model_version.model_name} / {model_version.version_name}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c2af27-4e00-488b-8c68-1cbe891aa204",
   "metadata": {
    "language": "sql",
    "name": "DORA_SEAI53"
   },
   "outputs": [],
   "source": [
    "-- SEAI53: Validate model version V2 was registered in Snowflake Model Registry\n",
    "SELECT util_db.public.se_grader(\n",
    "  'SEAI53',\n",
    "  (actual >= 1),\n",
    "  actual,\n",
    "  1,\n",
    "  '✅ Model Version V2 successfully registered!'\n",
    ") AS graded_results\n",
    "FROM (\n",
    "  SELECT COUNT(*) AS actual\n",
    "  FROM AICOLLEGE.INFORMATION_SCHEMA.MODEL_VERSIONS\n",
    "  WHERE MODEL_NAME = 'COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL'\n",
    "    AND MODEL_VERSION_NAME = 'V2'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b988d5-83f6-4c31-975c-f6434ae7bf4d",
   "metadata": {
    "collapsed": false,
    "name": "ModelMonitor"
   },
   "source": [
    "### 📈 Enable Model Monitoring with Snowflake ML Observability\n",
    "\n",
    "Now that both **Version 1** and **Version 2** of our **XGBoost mortgage response model** are registered, we’ll use **Snowflake ML Observability** to track and compare their real-world performance over time.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Why Model Monitoring Matters\n",
    "- 📊 Track critical metrics like **accuracy**, **precision**, **recall**, and **F1 score**\n",
    "- 🌀 Detect **prediction drift** and **feature drift** using recent inference batches\n",
    "- 🧠 Establish a **baseline** for expected model behavior\n",
    "- 📺 Monitor performance trends using **Snowsight’s built-in dashboards** — no external setup required\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Why We’re Re-Creating Monitors for V1 and V2\n",
    "\n",
    "We previously removed the original monitor for **Version 1**, but in order to use Snowsight’s **Compare Models** feature:\n",
    "\n",
    "- Each model version must have its **own active monitor**\n",
    "- We’ll now re-create both monitors to support side-by-side comparison between:\n",
    "  - ✅ **Version 1** (original XGBoost model)\n",
    "  - ✅ **Version 2** (retrained XGBoost with updated feature store data)\n",
    "\n",
    "---\n",
    "\n",
    "### 📂 Why Use `ALL_PREDICTIONS_WITH_GROUND_TRUTH`\n",
    "\n",
    "To ensure monitoring reflects **real-world inference behavior**, we’ll base both monitors on the `ALL_PREDICTIONS_WITH_GROUND_TRUTH` table:\n",
    "- ✅ Contains **actual predictions** made by deployed model versions\n",
    "- ✅ Includes **ground truth labels** for evaluation\n",
    "- ✅ Avoids leakage from training data or incomplete inference rows\n",
    "\n",
    "---\n",
    "\n",
    "This setup ensures transparent monitoring and lets us clearly assess whether Version 2 offers **consistent improvements** over Version 1 — not just in test performance, but in **ongoing model behavior in production**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40707fa2-3532-4549-9475-a307fea94266",
   "metadata": {
    "language": "sql",
    "name": "CreateModelMonitorV1"
   },
   "outputs": [],
   "source": "-- Recreate ML Observability Model Monitor for XGBoost (V1) model.\nCREATE OR REPLACE MODEL MONITOR MORTGAGE_MODEL_MONITOR_V1        -- Create for model version V1\nWITH \n  MODEL = AICOLLEGE.PUBLIC.COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL,    -- Provide required MLOPs HOL model name\n  VERSION = 'V1',\n  FUNCTION = 'predict',\n  SOURCE = AICOLLEGE.PUBLIC.ALL_PREDICTIONS_WITH_GROUND_TRUTH,   -- Full batch inference table\n  BASELINE = AICOLLEGE.PUBLIC.BASELINE_PREDICTIONS,              -- Baseline model or training sample\n  WAREHOUSE = AICOLLEGE,\n  REFRESH_INTERVAL = '365 DAY',\n  AGGREGATION_WINDOW = '7 DAYS',\n  TIMESTAMP_COLUMN = WEEK_START_DATE,\n  ID_COLUMNS = ('LOAN_ID'),\n  PREDICTION_CLASS_COLUMNS = ('PREDICTED_RESPONSE'),\n  ACTUAL_CLASS_COLUMNS = ('MORTGAGERESPONSE'),\n  PREDICTION_SCORE_COLUMNS = ('PREDICTED_SCORE');"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0ca2a-f756-4c8c-b071-a220231c5e8c",
   "metadata": {
    "language": "sql",
    "name": "CreateModelMonitorV2"
   },
   "outputs": [],
   "source": "-- Create new Model Monitor for the retrained XGBoost (V2) model.\nCREATE OR REPLACE MODEL MONITOR MORTGAGE_MODEL_MONITOR_V2\nWITH \n  MODEL = AICOLLEGE.PUBLIC.COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL,\n  VERSION = 'V2',\n  FUNCTION = 'predict',\n  SOURCE = AICOLLEGE.PUBLIC.V2_PREDICTIONS,                -- <-- V2 training predictions\n  BASELINE = AICOLLEGE.PUBLIC.BASELINE_PREDICTIONS,        -- <-- Baseline model\n  WAREHOUSE = AICOLLEGE,\n  REFRESH_INTERVAL = '365 DAY',\n  AGGREGATION_WINDOW = '7 DAYS',\n  TIMESTAMP_COLUMN = WEEK_START_DATE,\n  ID_COLUMNS = ('LOAN_ID'),\n  PREDICTION_CLASS_COLUMNS = ('PREDICTED_RESPONSE'),\n  ACTUAL_CLASS_COLUMNS = ('MORTGAGERESPONSE'),\n  PREDICTION_SCORE_COLUMNS = ('PREDICTED_SCORE');"
  },
  {
   "cell_type": "markdown",
   "id": "5a69962a-3cc1-4ecd-ad71-a289ecb3da60",
   "metadata": {
    "collapsed": false,
    "name": "ExploreModelMonitorDashboards"
   },
   "source": "### 🔍 Compare Model Versions in Snowsight\n\nNow that monitors are set up for **Version 1** and **Version 2**, use Snowsight’s **Compare** feature to view them side-by-side.\n\n1. In Snowsight, go to **AI & ML > Models**\n2. Select **`COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL`**\n3. Open either **V1** or **V2**\n4. Under **Monitors**, select **`MORTGAGE_MODEL_MONITOR`**\n5. Click the **“Compare”** toggle at the top of the monitor dashboard\n6. Select both **V1** and **V2**\n7. Set the date range to show **March 1, 2025 to July 20, 2025** of activity\n\nEven though both versions use the same XGBoost model architecture, this comparison view helps validate consistency and monitor subtle trends across model versions."
  },
  {
   "cell_type": "markdown",
   "id": "58ad711a-4522-4983-8ad6-9de400e009f1",
   "metadata": {
    "collapsed": false,
    "name": "MLExplainability"
   },
   "source": "### 🔍 Enable Model Explainability with Snowflake `EXPLAIN`\n\nWith our model now deployed and generating predictions, we can use **Snowflake’s built-in `EXPLAIN` function** to understand how input features contribute to model outputs — directly in Snowflake, without requiring any external libraries.\n\nRunning `EXPLAIN` returns a table of feature contributions for each row, showing how much each input drove the model’s decision.\n\nThis produces **SHAP-style contribution scores** for each input feature, letting you quantify exactly how much each column nudged the model’s decision:\n\n✅ Supported for models trained using **Snowflake ML** (e.g., XGBoost, LightGBM, Scikit-learn)  \n✅ Provides per-feature contribution scores for individual predictions  \n✅ Seamlessly integrated into **Python or SQL** workflows\n\nIn the next step, we’ll generate explain outputs for both **Version 1** and **Version 2** of our model and visualize how feature contributions shift between them."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cda0fb-26ca-4625-be29-7d490ccefefe",
   "metadata": {
    "language": "python",
    "name": "SnowflakeMLExplainability"
   },
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Step 1 — Run the EXPLAIN function on a sample input\nmodel_version = registry.get_model(\"COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL\").version(\"V2\")\nexplanations_df = model_version.run(\n    X_train.sample(n=50, random_state=42),\n    function_name=\"EXPLAIN\"  # --> Use Snowflake's new EXPLAIN function\n)\n\n# Step 2 — Identify explanation columns (excluding OHE columns)\nexplanation_cols = [\n    col for col in explanations_df.columns\n    if col.endswith(\"_explanation\") and \"_OHE_\" not in col\n]\n\n# Step 3 — Horizontal Bar Plot (Snowflake Explain)\nmean_abs_explanations = explanations_df[explanation_cols].abs().mean().sort_values(ascending=True)\n\nplt.figure(figsize=(4, 2))\nmean_abs_explanations.plot(kind=\"barh\")\nplt.title(\"SHAP-style Bar Plot (Snowflake EXPLAIN)\")\nplt.xlabel(\"Mean |Contribution|\")\nplt.ylabel(\"Feature\")\nplt.show()\n\n# Step 4 — Beeswarm-style Dot Plot\ndot_data = explanations_df[explanation_cols].copy()\ndot_data.columns = [col.replace(\"_explanation\", \"\") for col in dot_data.columns]\ndot_data = dot_data.melt(var_name=\"Feature\", value_name=\"Contribution\")\n\nplt.figure(figsize=(4, 2))\nsns.stripplot(\n    data=dot_data,\n    x=\"Contribution\",\n    y=\"Feature\",\n    size=4,\n    alpha=0.6,\n    jitter=True,\n    orient=\"h\"\n)\nplt.title(\"Beeswarm-style Dot Plot (Snowflake EXPLAIN)\")\nplt.xlabel(\"Contribution Value\")\nplt.ylabel(\"Feature\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "f2939d58-3d26-4295-b185-6457345af275",
   "metadata": {
    "collapsed": false,
    "name": "SHAPSummary"
   },
   "source": [
    "### 📊 Explainability Output (Snowflake `EXPLAIN` Function)\n",
    "\n",
    "These charts show the **contribution of numeric input features** (excluding one-hot-encoded columns) as derived using Snowflake’s built-in `EXPLAIN` capability.\n",
    "\n",
    "### 🔵 Top Chart: Mean Absolute Contribution (Bar Plot)\n",
    "- Shows the **average absolute contribution** for each numeric feature across 50 examples.\n",
    "- Longer bars indicate **greater overall influence** on the model’s prediction.\n",
    "- In this case, `LOAN_AMOUNT_000S` has the largest impact, followed by `APPLICANT_INCOME_000S`.\n",
    "\n",
    "### 🔴 Bottom Chart: Dot Plot (Beeswarm-style)\n",
    "- Displays each example’s **raw contribution value** on the x-axis.\n",
    "- The **vertical spread** shows variability across inputs; cluster density indicates concentration of effects.\n",
    "- Useful to visualize how certain features push predictions **positively or negatively**.\n",
    "\n",
    "Together, these plots improve transparency by surfacing **how much and in which direction** the input features are influencing predictions in your retrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a8e84-953b-4ac2-af10-87113eeba74e",
   "metadata": {
    "collapsed": false,
    "name": "MLOpsModelPromotionViaAliases"
   },
   "source": [
    "### ✅ Promote and Use Production Model with Aliases\n",
    "Now that we’ve registered multiple versions of our model, we’ll adopt a lifecycle strategy using aliases — a flexible and production-ready approach supported by the **Snowflake Model Registry**.\n",
    "\n",
    "Instead of hardcoding version numbers (e.g., V1, V2), we assign an alias like production to the latest validated model version. This enables us to:\n",
    "- 🔄 Seamlessly promote new versions without updating downstream code\n",
    "- 🔒 Maintain cleaner, version-agnostic pipeline logic\n",
    "- ⚙️ Support rollback or staged rollout by shifting the alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60830013-75cc-4ef6-97be-af5cd46765de",
   "metadata": {
    "language": "sql",
    "name": "DecommissionV1"
   },
   "outputs": [],
   "source": "-- Assign alias 'DECOMMISSIONED' to V1 for tracking purposes\nALTER MODEL COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL VERSION V1 SET XXX = DECOMMISSIONED;   -- Use Snowflake's ALIAS model parameter\n\n-- Assign alias 'PRODUCTION' to V2\nALTER MODEL COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL VERSION V2 SET XXX = XXX;       -- Set V2 as alias = PRODUCTION"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c532121b-fb19-411c-9f64-c804af949601",
   "metadata": {
    "language": "sql",
    "name": "DORA_SEAI54"
   },
   "outputs": [],
   "source": "-- SEAI54: Validate that Version V2 has the PRODUCTION alias\nSELECT util_db.public.se_grader(\n  'SEAI53',\n  (actual >= 1),\n  actual,\n  1,\n  '✅ Model Version V2 successfully registered!'\n) AS graded_results\nFROM (\n  SELECT COUNT(*) AS actual\n  FROM AICOLLEGE.INFORMATION_SCHEMA.MODEL_VERSIONS\n  WHERE MODEL_NAME = 'COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL'\n    AND MODEL_VERSION_NAME = 'V2'\n);"
  },
  {
   "cell_type": "markdown",
   "id": "43180de9-bfa9-47e3-8d92-fdde46a512c6",
   "metadata": {
    "collapsed": false,
    "name": "ModelRollBack"
   },
   "source": [
    "### 🔄 Roll Back Production Alias to V1\n",
    "\n",
    "If you ever need to roll back your `production` alias from V2 back to V1, run these steps **after** your example pipelines:\n",
    "\n",
    "```sql\n",
    "-- 1) Remove the PRODUCTION alias from V2\n",
    "ALTER MODEL AICOLLEGE.PUBLIC.COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL\n",
    "  VERSION V2\n",
    "  UNSET ALIAS;\n",
    "\n",
    "-- 2) Clear any DECOMMISSIONED or other alias on V1\n",
    "ALTER MODEL AICOLLEGE.PUBLIC.COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL\n",
    "  VERSION V1\n",
    "  UNSET ALIAS;\n",
    "\n",
    "-- 3) Assign the PRODUCTION alias back to V1\n",
    "ALTER MODEL AICOLLEGE.PUBLIC.COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL\n",
    "  VERSION V1\n",
    "  SET ALIAS = PRODUCTION;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27edffba-4a5a-4222-ab8d-31762f2084e3",
   "metadata": {
    "collapsed": false,
    "name": "ScorePromotedModelExamples"
   },
   "source": [
    "### Score with the Promoted Production Model\n",
    "\n",
    "With multiple model versions registered, we now use the **PRODUCTION** alias to serve both class labels and probability scores—without ever hard-coding a version number.\n",
    "\n",
    "This example shows how to:\n",
    "\n",
    "- Pull recent rows from the cleaned & encoded training data (`NEWTRAININGDATA_FINAL`)\n",
    "- Join with the Feature Store (`LOAN_FEATURES_OHE$1`)\n",
    "- Dynamically score new data using the latest production model, capturing:\n",
    "  - 🔢 **SCORE**: the model’s probability that the positive class occurs  \n",
    "  - 🔢 **PREDICTION**: the hard 0/1 label\n",
    "- Persist a single table with both fields for downstream reporting or monitoring\n",
    "\n",
    "By binding downstream code to the **PRODUCTION** alias you get:\n",
    "\n",
    "- 🔄 **Version-agnostic pipelines** — no edits when you retrain or promote  \n",
    "- 🔐 **Instant rollback** — simply retarget the alias if you discover an issue  \n",
    "- ⚙️ **Plug-and-play automation** (Tasks, dbt, dynamic tables, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b24b1-a69d-4d03-8f37-f67058fa651a",
   "metadata": {
    "language": "python",
    "name": "ExampleBatchScoring"
   },
   "outputs": [],
   "source": "from snowflake.ml.registry import Registry\nfrom snowflake.snowpark.functions import col\nfrom snowflake.snowpark import Session\n\n# --- Initialize Registry and Get Model Version ---\nreg = Registry(session=session, database_name=\"AICOLLEGE\", schema_name=\"PUBLIC\")\nmodel_prod = reg.get_model(\"COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL\").version(\"production\")\n\n# --- Load Feature Store Data ---\ndf_features = session.table(\"AICOLLEGE.PUBLIC.LOAN_FEATURES_OHE$1\")\n\n# --- Drop rows with NULLs before inference ---\ndf_features_clean = df_features.dropna()\n\n# --- Run prediction using PREDICT_PROBA ---\ndf_scores = model_prod.run(df_features_clean, function_name=\"PREDICT_PROBA\")\n\n# --- Safely check output feature names ---\nprint(\"Returned columns:\", df_scores.columns)\n\n# --- Build final prediction DataFrame ---\ndf_final = (\n    df_scores\n    .with_column(\"PROBABILITY_SCORE\", col('\"output_feature_1\"'))  # use double quotes to handle Snowflake case sensitivity\n    .with_column(\"PREDICTION_LABEL\", (col('\"output_feature_1\"') > 0.5).cast(\"INT\"))\n)\n\n# --- Save predictions ---\ndf_final.write.mode(\"overwrite\").save_as_table(\"AICOLLEGE.PUBLIC.MORTGAGE_PREDICTIONS_PROD\")\n\nprint(\"✅ Scoring pipeline completed successfully\")"
  },
  {
   "cell_type": "markdown",
   "id": "de2deea5-05aa-4378-ad14-f455c755ff0d",
   "metadata": {
    "name": "HOLFeedback"
   },
   "source": [
    "### 🧠 Feedback is Fuel  \n",
    "We’d love your input to help improve this lab! Please take 3 minutes to fill out our feedback form:  \n",
    "👉 [Submit Feedback Here](https://docs.google.com/forms/d/e/1FAIpQLSdJwMBSnYffjJI5N9db3IjHz36dMtcpvI1YmQuMwO0EgAUS8A/viewform?usp=header)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bdff86-5d66-4cc6-a565-3a0c18c2d3cb",
   "metadata": {
    "collapsed": false,
    "name": "HOLCleanup"
   },
   "source": [
    "### 🧹 HOL Cleanup\n",
    "\n",
    "To reset your environment and avoid lingering monitors or scheduled alerts, you can drop the model monitor and alert below.\n",
    "\n",
    "This is especially useful when:\n",
    "\n",
    "- You're sharing an environment with other users\n",
    "- You plan to rerun the notebook from the top\n",
    "- You want to avoid refresh or alert-triggering compute\n",
    "\n",
    "⚠️ Skip this section if you're actively monitoring your model in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c6cbbf-80fa-48a8-abd3-78df4df00773",
   "metadata": {
    "language": "sql",
    "name": "Cleanup"
   },
   "outputs": [],
   "source": "-- HOL CLEANUP SCRIPT\n\n-- Drop model monitors first (they may reference the model)\nDROP MODEL MONITOR IF EXISTS MORTGAGE_MODEL_MONITOR_V1;\nDROP MODEL MONITOR IF EXISTS MORTGAGE_MODEL_MONITOR_V2;\n\n-- -- Drop the ML model\nDROP MODEL IF EXISTS COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL;\n\n-- Drop dynamic feature table (feature store)\nDROP DYNAMIC TABLE IF EXISTS LOAN_FEATURES_OHE$1;\n\n-- Drop the Feature View (this is a regular view or dynamic table)\nDROP VIEW IF EXISTS AICOLLEGE.PUBLIC.LOAN_FEATURES_OHE;\n\n-- Drop the Entity (this is actually a TAG in Feature Store)\nDROP TAG IF EXISTS AICOLLEGE.PUBLIC.LOAN_ENTITY;\n\n-- Drop intermediate and prediction tables\nDROP TABLE IF EXISTS ALL_PREDICTIONS_WITH_GROUND_TRUTH;\nDROP TABLE IF EXISTS BASELINE_PREDICTIONS;\nDROP TABLE IF EXISTS INFERENCEMORTGAGEDATA;\nDROP TABLE IF EXISTS MORTGAGE_PREDICTIONS_PROD;\nDROP TABLE IF EXISTS NEWTRAININGDATA;\nDROP TABLE IF EXISTS NEWTRAININGDATA_CLEANED;\nDROP TABLE IF EXISTS NEWTRAININGDATA_FINAL;\nDROP TABLE IF EXISTS PREDICTIONS_WITH_GROUND_TRUTH;\nDROP VIEW IF EXISTS AICOLLEGE.PUBLIC.V2_PREDICTIONS;\nDROP TABLE IF EXISTS AICOLLEGE.PUBLIC.V2_RAW_PREDICTIONS;\n\n-- Drop any custom file formats used\nDROP FILE FORMAT IF EXISTS MLOPS;"
  }
 ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b61b119c-2f1d-48f4-8707-53f376d7848c",
   "metadata": {},
   "source": [
    "# Step 1: Read Model Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf9e5f-d341-411c-8eab-7ed3074a1346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import s3fs\n",
    "\n",
    "# Set your bucket and file path\n",
    "bucket_name = 'college-of-ai'\n",
    "file_key = 'MORTGAGE_LENDING_DEMO_DATA.csv'\n",
    "\n",
    "# Read CSV directly from S3\n",
    "df = pd.read_csv(f's3://{bucket_name}/{file_key}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e161f5-88b7-4835-ada8-029313e49b57",
   "metadata": {},
   "source": [
    "# Step 2: Import Libraries & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322054fb-3fa4-4349-8454-c4d2aff5c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Drop rows with missing target or required numeric/categorical values\n",
    "df = df.dropna(subset=[\n",
    "    \"MORTGAGERESPONSE\", # <-- This is your target variable?\n",
    "    \"APPLICANT_INCOME_000S\",\n",
    "    \"LOAN_AMOUNT_000S\",\n",
    "    \"LOAN_TYPE_NAME\", # <-- This is a categorical column\n",
    "    \"LOAN_PURPOSE_NAME\", # <-- This is a categorical column\n",
    "    \"COUNTY_NAME\" # <-- This is a categorical column\n",
    "])\n",
    "\n",
    "# One-hot encode selected categorical columns\n",
    "categorical_cols = [\"LOAN_TYPE_NAME\", \"XXX\", \"XXX\"] # <-- Add the remaining categorical columns\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Filter to numeric columns only (drop anything suspicious like strings/dates)\n",
    "X = df_encoded.drop(columns=[\"XXX\"]) # <-- Which column is the target variable?\n",
    "X = X.select_dtypes(include=[\"number\"]).copy()\n",
    "\n",
    "# Drop rows with NaNs if any remain\n",
    "X = X.dropna()\n",
    "y = df_encoded.loc[X.index, \"XXX\"] # <-- Match this to your target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96693f2b-5e92-48d9-8696-d90a43ed2992",
   "metadata": {},
   "source": [
    "# Step 3: Split into Train, Validation, and Holdout Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a394c-576d-41db-a2c1-4e44696399ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: 80% temp (train + val), 20% holdout (0.2 is your first test_size)\n",
    "X_temp, X_holdout, y_temp, y_holdout = train_test_split(X, y, test_size=XXX, random_state=42, stratify=y)\n",
    "\n",
    "# Second split: 75% train, 25% val (0.25 is your second test_size)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=XXX, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Validation: {len(X_val)}, Holdout: {len(X_holdout)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03a656e-a241-4332-bcc7-779f4b04bacc",
   "metadata": {},
   "source": [
    "# Step 4: Train XGBoost Model using 'logloss' as the evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea7c18-6895-4abc-b062-1ae685165d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'logloss' penalizes incorrect predictions with high confidence‚Äîideal for binary classification.\n",
    "model = XGBClassifier(eval_metric='logloss')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87747bb3-19e8-47de-9785-fe34e9a0a0d8",
   "metadata": {},
   "source": [
    "# Step 5: Evaluate Model Performance on the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcedc618-fe5e-400e-813f-f41808ecdb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained XGBoost model to generate predictions on the validation set.\n",
    "# These predictions will help us assess how well the model generalizes to unseen data.\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Print a detailed classification report including precision, recall, F1-score, and support.\n",
    "# This gives us insight into the model‚Äôs ability to correctly classify each class.\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_pred))\n",
    "\n",
    "# Print the confusion matrix to visualize the number of true positives, false positives,\n",
    "# true negatives, and false negatives. This helps understand model performance at a glance.\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
    "\n",
    "# NOTE: We will also precompute and store these metrics in Step 9 when registering the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b587db-24ca-496b-a417-0a7e477ac3fb",
   "metadata": {},
   "source": [
    "# Step 6: Install Required Snowflake Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f08c00-e6de-43d5-b5d4-0099817287a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Snowflake ML Python library\n",
    "# NOTE: You may see some warnings about other packages (e.g., dask, jupyter-ai). \n",
    "# These are safe to ignore for this lab and won't impact functionality.\n",
    "!pip install snowflake-ml-python --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f852e52-c07f-42ec-bed0-b084355d2632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Snowpark Python library.\n",
    "# This is used to connect to Snowflake and work with data using Python DataFrame operations.\n",
    "!pip install snowflake-snowpark-python --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d435a369-9322-46b0-b00d-df7361521c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries:\n",
    "# - `toml` is used to read the Snowflake connection info from a `connections.toml` file.\n",
    "# - `snowflake-ml-python` provides tools for model training, registration, and scoring in Snowflake.\n",
    "!pip install toml snowflake-ml-python --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157018d5-0d29-4a87-91cf-c3ca6d416d1c",
   "metadata": {},
   "source": [
    "# Step 7: Allow SageMaker to Access Snowflake\n",
    "In order for your SageMaker instance to connect to Snowflake, you must **add the current public IP address** of this notebook to your Snowflake network policy.\n",
    "\n",
    "### Step 7A: Get the Public IP of this SageMaker Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dc2316-2cff-45ca-bc4a-2507afa73cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following cell in your notebook:\n",
    "!curl ifconfig.me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f521dc-6286-4b51-9ff6-acba4807b107",
   "metadata": {},
   "source": [
    "### Step 7B: Add the IP to Your Snowflake Network Policy\n",
    "\n",
    "Copy the Step 7A output (ex: `52.183.42.53`) and update your Snowflake network policy by running the following SQL in Snowsight (you must have `ACCOUNTADMIN` privileges):\n",
    "\n",
    "#### Option 1: Create a new network policy (recommended if not already created)\n",
    "```sql\n",
    "-- Create it once\n",
    "CREATE OR REPLACE NETWORK POLICY ALLOW_SAGEMAKER\n",
    "  ALLOWED_IP_LIST = ('<YOUR_SAGEMAKER_IP>')\n",
    "  COMMENT = 'Restrict access to SageMaker IPs for MLOps HOL';\n",
    "-- Assign to service user only\n",
    "ALTER USER mlops_user SET NETWORK_POLICY = ALLOW_SAGEMAKER;\n",
    "-- Verify assignment\n",
    "DESC USER mlops_user;\n",
    "```\n",
    "#### Option 2: Append to an existing IP allowlist (preserve existing IPs)\n",
    "```sql\n",
    "-- If you already have other IPs in the list, you can append your SageMaker IP like this:\n",
    "ALTER NETWORK POLICY ALLOW_SAGEMAKER SET ALLOWED_IP_LIST = (\n",
    "  'existing.ip.1',\n",
    "  'existing.ip.2',\n",
    "  '52.183.42.53'\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d4a4c-20ac-42ee-a80c-a5a694f2ea47",
   "metadata": {},
   "source": [
    "# Step 8: Connect to Snowflake from SageMaker using a .toml File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545e26ee-3ae7-4a26-af62-7bd884ff0368",
   "metadata": {},
   "source": [
    "This notebook uses a local .toml file to securely store your Snowflake connection parameters.\n",
    "\n",
    "### Step 8A: Create a connections.toml file\n",
    "Create a connections.toml file locally with the following structure:\n",
    "```\n",
    "[connections.snowflake_conn]\n",
    "account = \"your_account\"\n",
    "user = \"mlops_user\"\n",
    "role = \"aicollege\"\n",
    "warehouse = \"aicollege\"\n",
    "database = \"aicollege\"\n",
    "schema = \"public\"\n",
    "authenticator = \"snowflake_jwt\"\n",
    "```\n",
    "üí° Replace the values with your actual Snowflake connection details.\n",
    "\n",
    "üìù Reminder: After updating the connections.toml file with your Snowflake account details, make sure to save the file before running the next step.\n",
    "Otherwise, your notebook won‚Äôt be able to read the correct connection settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe3e31f-b016-4ffd-9c0d-a223b18a1aa9",
   "metadata": {},
   "source": [
    "### Step 8B: Upload the `.toml` and `.pem` Files to SageMaker\n",
    "\n",
    "In the left sidebar of this JupyterLab environment, click the üìÅ **File Browser** icon.\n",
    "\n",
    "Right-click ‚Üí **Upload Files** and choose both of the following files:\n",
    "- `connections.toml` (your Snowflake connection config)\n",
    "- `rsa_private_key.pem` (your private key for key-pair authentication)\n",
    "\n",
    "‚úÖ Confirm both files appear in the file tree (e.g., `/connections.toml` and `/rsa_private_key.pem`).\n",
    "\n",
    "üõ†Ô∏è If your `.toml` references a custom path (like a `keys/` folder), ensure the `.pem` file is uploaded to that same path or adjust the `private_key_path` value accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74300d3-e676-40a6-ad54-a35d52f4aa4c",
   "metadata": {},
   "source": [
    "### Step 8C: Validate your Snowpark with connections.toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87816bb-dbac-48bd-834c-da2c6022f283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "# Load the Snowflake connection config\n",
    "config = toml.load(\"XXX\") # <-- Provide path to your connections.toml file\n",
    "params = config[\"connections\"][\"XXX\"] # <-- Specify the connection key\n",
    "\n",
    "# Manually load the private key\n",
    "with open(\"rsa_private_key.pem\", \"rb\") as key_file:\n",
    "    private_key = serialization.load_pem_private_key(key_file.read(), password=None)\n",
    "\n",
    "# Inject the private key into params\n",
    "params[\"private_key\"] = private_key\n",
    "\n",
    "# Create a Snowpark session\n",
    "session = Session.builder.configs(params).create()\n",
    "\n",
    "# Test the connection\n",
    "session.sql(\"SELECT current_user(), current_warehouse(), current_database(), current_schema()\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364ee8c8-c4bd-435c-b185-f86bc6148145",
   "metadata": {},
   "source": [
    "# Step 9: Register SageMaker Model in Snowflake Model Registry\n",
    "After training a model in SageMaker, you can register it directly in Snowflake using the `log_model()` API ‚Äî **no need to upload a `.pkl` file to a stage**.\n",
    "\n",
    "When you register a model in the Snowflake Model Registry:\n",
    "- You pass the **Python model object directly** (e.g., XGBoost, scikit-learn) ‚Äî no manual artifact staging required.\n",
    "- The model is **automatically serialized, versioned, and stored as a first-class Snowflake object**.\n",
    "- **Model metadata** ‚Äî including name, version, dependencies, metrics, tags, and sample input ‚Äî is **captured at registration**.\n",
    "- You can **run inference at scale using Python or SQL** via `.run()` on any Snowflake table with compatible features.\n",
    "- You can **track model versions, update tags and metrics, and monitor performance or drift** using **Snowflake ML Observability**.\n",
    "\n",
    "The Snowflake Model Registry provides:\n",
    "- **Built-in support for common ML frameworks** (e.g., XGBoost, scikit-learn, LightGBM, TensorFlow, PyTorch, Hugging Face, MLflow)\n",
    "- **Role-based access control** (RBAC) and **schema-level organization**\n",
    "- **Lifecycle management** from dev to prod\n",
    "- **Secure sharing and governance** for ML teams\n",
    "\n",
    "üí° Tip: Every Snowflake schema can act as a model registry. It's recommended to use a dedicated schema to organize your models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524411e-1e9b-419d-a309-23e61d3744c8",
   "metadata": {},
   "source": [
    "### Step 9A: Create Small Input Data for Snowflake Model Registry\n",
    "It helps Snowflake infer the input schema so it knows how to call the model later (during inference, for example). \n",
    "\n",
    "Using a small sample is a best practice to keep things fast and lightweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd87895-7b1c-4641-802b-4ff5e8078188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a small sample from your training data (cast to float32) for model signature inference\n",
    "sample_input_data = XXX # <-- Replace with training DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acd0f34-6fbe-4f42-a6b6-22822e7cce03",
   "metadata": {},
   "source": [
    "### Step 9B: Precompute metrics to log to the Snowflake Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f2dc60-ca7f-41cc-88fd-1dbdfc2b1cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture model training metrics to include in Snowflake Model Registry log\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "acc = accuracy_score(y_val, y_pred)                           # <-- scalar metric\n",
    "f1 = f1_score(y_val, y_pred)                                  # <-- scalar metric\n",
    "report = classification_report(y_val, y_pred, output_dict=True)  # <-- JSON-serializable\n",
    "cmatrix = confusion_matrix(y_val, y_pred).tolist()            # <-- list format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88ac218-7bed-414f-8a53-caf6eeeb22fc",
   "metadata": {},
   "source": [
    "### Step 9C: Initialize the Snowflake Model Registry object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a7bdcc-3723-413f-aff1-8eb9e67c1d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This connects to the schema where the model will be stored\n",
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "registry = Registry(                               \n",
    "    session=session,                                # <-- your Snowpark session\n",
    "    database_name='AICOLLEGE',                      # <-- your model database\n",
    "    schema_name='PUBLIC'                            # <-- your model schema\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaf14ad-23fc-4652-987e-54ab0ca799e0",
   "metadata": {},
   "source": [
    "### Step 9D: Log the trained model to Snowflake Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e16eb-d89a-4deb-814d-5837e43790b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This serializes the model, stores it in Snowflake, and adds initial metadata\n",
    "from snowflake.ml.model import type_hints\n",
    "\n",
    "model_version = registry.log_model(\n",
    "    model=model,                                      # <-- Provide your trained XGBClassifier\n",
    "    model_name='XXX',   # <-- Provide required MLOPs HOL model name\n",
    "    version_name='XXX',                                # <-- Provide model version name\n",
    "    sample_input_data=sample_input_data,              # <-- sample input for schema inference\n",
    "    conda_dependencies=['XXX'],                   # <-- Provide the predictive model dependencies\n",
    "    comment=\"\"\"XGBoost classifier trained in SageMaker to predict mortgage response.\n",
    "This version uses 'logloss' as the evaluation metric and includes precomputed model metrics.\"\"\",\n",
    "    metrics={\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_score\": f1,\n",
    "        \"classification_report\": report,\n",
    "        \"confusion_matrix\": cmatrix\n",
    "    },\n",
    "    task=type_hints.Task.TABULAR_BINARY_CLASSIFICATION  # <-- Snowflake ML Observability\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a1d8de-f15f-4b3e-b9c7-6dbf04d56944",
   "metadata": {},
   "source": [
    "### Step 9E: Create and Apply Tags for Model Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322188c8-474f-4107-b938-9465c5df65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optional tags for better governance and tracking\n",
    "# Run once per schema (add missing ones if needed)\n",
    "session.sql(\"CREATE OR REPLACE TAG MODEL_STAGE_TAG\").collect()\n",
    "session.sql(\"CREATE OR REPLACE TAG MODEL_PURPOSE_TAG\").collect()\n",
    "session.sql(\"CREATE OR REPLACE TAG SOURCE\").collect()\n",
    "session.sql(\"CREATE OR REPLACE TAG PROJECT\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfac92d3-69b2-4939-aded-75323653a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tags and model-level metadata\n",
    "m = registry.get_model('COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL')   # <-- Provide required MLOPs HOL model name\n",
    "\n",
    "# Add model-level description\n",
    "m.comment = \"XGBoost model to predict mortgage approval. Trained in SageMaker and registered in Snowflake.\"\n",
    "\n",
    "# Add model-level tags\n",
    "m.set_tag(\"MODEL_STAGE_TAG\", \"PROD\")                                 # <-- deployment_stage\n",
    "m.set_tag(\"MODEL_PURPOSE_TAG\", \"Mortgage Response Classification\")   # <-- business_context\n",
    "m.set_tag(\"SOURCE\", \"SageMaker\")                                     # <-- origin\n",
    "m.set_tag(\"PROJECT\", \"College of AI - MLOps HOL\")                    # <-- HOL traceability\n",
    "\n",
    "# View tags\n",
    "m.show_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500a2689",
   "metadata": {},
   "source": [
    "### üéì DORA Grading Setup Reminder\n",
    "If you haven't configured the `util_db.public.se_grader` function yet, please ensure you've completed the DORA setup steps outlined in the **[College of AI HOL Setup instructions](https://docs.google.com/document/d/1z-CG06Kt2dzV2bLxTDsP55qxQY6lzwtSDE5Azz4IwJs/edit?tab=t.0#heading=h.vgy1lc5t2roq)**.\n",
    "\n",
    "If you're unsure whether it's already configured:\n",
    "- Run `SHOW INTEGRATIONS;` to check for `dora_api_integration`.\n",
    "- If missing, follow the provided setup script.\n",
    "\n",
    "Once confirmed, proceed with the DORA evaluation steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc381ab-95e8-4759-9d53-8838c68430e2",
   "metadata": {},
   "source": [
    "# Step 10: Evaluation Test # 50 Sagemaker Model Registration in Snowflake (SEAI50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13416ad3-2988-43b7-858f-5d4c95d5c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEAI50: Validate model was registered in Snowflake Model Registry\n",
    "query = \"\"\"\n",
    "WITH models AS (\n",
    "  SELECT \"name\" AS model_name\n",
    "  FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))\n",
    ")\n",
    "SELECT util_db.public.se_grader(\n",
    "    'SEAI50',\n",
    "    (actual >= 1),\n",
    "    actual,\n",
    "    1,\n",
    "    'Your model has been registered successfully!'\n",
    ") AS graded_results\n",
    "FROM (\n",
    "    SELECT COUNT(*) AS actual\n",
    "    FROM models\n",
    "    WHERE model_name ILIKE '%COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL%'\n",
    ")\n",
    "\"\"\"\n",
    "# First run SHOW MODELS with a separate call\n",
    "session.sql(\"SHOW MODELS IN DATABASE AICOLLEGE\").collect()\n",
    "\n",
    "# Then run the grading query\n",
    "session.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aaab22-bfa8-432c-ae40-b5b12cf08c35",
   "metadata": {},
   "source": [
    "# Step 11: Preprocess Inference Data for Batch Scoring\n",
    "\n",
    "Before we run batch inference, we need to make sure the incoming Snowflake data matches the **feature format the model was trained on**.\n",
    "\n",
    "Our SageMaker model expects:\n",
    "- All numeric features\n",
    "- One-hot encoded columns for `LOAN_TYPE_NAME`, `LOAN_PURPOSE_NAME`, and `COUNTY_NAME`\n",
    "- The same column names and order as the training data\n",
    "\n",
    "In this step, we:\n",
    "- Load `InferenceMortgageData` from Snowflake\n",
    "- Apply one-hot encoding using Pandas\n",
    "- Fill in any missing columns with 0s\n",
    "- Reorder columns to match the model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c3a7fb-97cd-496e-a464-b763d086ac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Week 1 data from Snowflake into a Pandas DataFrame\n",
    "raw_input = session.table(\"XXX\").filter(\"XXX\").to_pandas() # <-- Provide inference data table and filter for WEEK = 1\n",
    "\n",
    "# Apply one-hot encoding on the same columns used during training\n",
    "categorical_cols = [\"LOAN_TYPE_NAME\", \"XXX\", \"XXX\"] # <-- Provide required categorical column names for the required preprocessing step\n",
    "encoded_input = pd.get_dummies(raw_input, columns=categorical_cols)\n",
    "\n",
    "# Align the input with model training features (X_train.columns)\n",
    "for col in X_train.columns:\n",
    "    if col not in encoded_input.columns:\n",
    "        encoded_input[col] = 0  # Add missing columns with default 0\n",
    "\n",
    "# Reorder the columns to match training\n",
    "encoded_input = encoded_input[X_train.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb169c4d-d088-4fc6-bebb-99abe11a086b",
   "metadata": {},
   "source": [
    "# Step 12: Week 1 Mortgage Application Scoring\n",
    "\n",
    "Mortgage application data is continuously collected and stored in Snowflake.\n",
    "Rather than scoring in real-time, we use **batch inference** to evaluate applications in bulk ‚Äî a common approach when scoring latency is not critical.\n",
    "\n",
    "In this step, you will:\n",
    "- **Run inference** on Week 1 application data using the registered XGBoost model\n",
    "- **Format and rename the prediction output** for clarity\n",
    "- **Join predictions with the true outcome** (MORTGAGERESPONSE) for monitoring\n",
    "- **Save the results in a unified table** (PREDICTIONS_WITH_GROUND_TRUTH) for downstream analysis and model monitoring\n",
    "\n",
    "This unified table enables better model observability and drift detection by tracking how model predictions align with true outcomes over time.\n",
    "\n",
    "üß† **Reminder:** Your SageMaker model expects the input features to be preprocessed exactly as they were during training. We‚Äôve already one-hot encoded and aligned the inference data in Step 13 to match that format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7549a6ff-54b2-402c-9687-28bfc195ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Batch Inference on Week 1 Data and Save Unified Output Table\n",
    "\n",
    "from snowflake.ml.registry import Registry\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the Model Registry\n",
    "registry = Registry(session=session, database_name=\"AICOLLEGE\", schema_name=\"PUBLIC\")\n",
    "model = registry.get_model(\"XXX\")  # <-- Match your registered model name\n",
    "model_version = model.version(\"XXX\")  # <-- Match your version model name\n",
    "\n",
    "# Run inference on encoded data\n",
    "predictions = model_version.run(encoded_input, function_name=\"predict\")\n",
    "proba_predictions = model_version.run(encoded_input, function_name=\"predict_proba\")\n",
    "\n",
    "# Create prediction columns\n",
    "pred_series = pd.Series(np.squeeze(predictions), name=\"XXX\")  # <-- Provide model prediction response name\n",
    "score_series = pd.Series(np.array(proba_predictions)[:, 1], name=\"XXX\")  # <-- Provide model prediction score name\n",
    "\n",
    "# Combine with original raw input for observability\n",
    "results_df = raw_input.copy()\n",
    "results_df[\"WEEK\"] = 1\n",
    "results_df[\"PREDICTED_RESPONSE\"] = pred_series\n",
    "results_df[\"PREDICTED_SCORE\"] = score_series\n",
    "\n",
    "# Convert to Snowpark DataFrame and save to Snowflake\n",
    "results = session.create_dataframe(results_df)\n",
    "results.write.mode(\"overwrite\").save_as_table(\"XXX\")  # <-- Provide name of saved table with model prediction responses\n",
    "\n",
    "# Preview results\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e86d0-8cfb-4fd0-8e93-22692bd08c4f",
   "metadata": {},
   "source": [
    "# Step 13: Weekly Batch Scoring (Weeks 2‚Äì5)\n",
    "\n",
    "To simulate ongoing model usage, we now run **batch inference** on new mortgage application data for **weeks 2 through 5** using the same model and preprocessing logic applied to week 1.\n",
    "\n",
    "In this step, you will:\n",
    "- **Load raw application data** for each week from Snowflake\n",
    "- **Apply the same one-hot encoding logic** used during training\n",
    "- **Ensure feature alignment** with the model input (add missing columns, reorder)\n",
    "- **Run inference** using the registered SageMaker model\n",
    "- **Join predictions with actual outcomes** (`MORTGAGERESPONSE`)\n",
    "- **Append results** to the unified output table: `PREDICTIONS_WITH_GROUND_TRUTH`\n",
    "\n",
    "üß† **Note:** For model scoring to be accurate, your inference data must use the same preprocessing steps (like one-hot encoding) and column structure used during training.\n",
    "\n",
    "This mirrors a common MLOps pattern, where **new incoming data** is scored in batches on a regular schedule and **stored in Snowflake** for:\n",
    "- Dashboards & business reporting\n",
    "- Model monitoring and drift detection\n",
    "- Performance evaluation over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27191177-3689-455e-9d3d-54eaf5816258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Batch Inference on Weeks 2‚Äì5 and Update Unified Output Table\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "for week in range(2, 6):  # Weeks 2 through 5\n",
    "    print(f\"üîÑ Processing week {week}...\")\n",
    "\n",
    "    # Load raw inference data from Snowflake\n",
    "    raw_week_df = session.table(\"InferenceMortgageData\").filter(f\"WEEK = {week}\").to_pandas()\n",
    "\n",
    "    # Apply one-hot encoding to categorical columns\n",
    "    encoded_week_df = pd.get_dummies(raw_week_df, columns=categorical_cols)\n",
    "\n",
    "    # Add any missing columns expected by the model\n",
    "    for col in X_train.columns:\n",
    "        if col not in encoded_week_df.columns:\n",
    "            encoded_week_df[col] = 0  # Add missing columns with default value 0\n",
    "\n",
    "    # Reorder columns to match the training feature order\n",
    "    encoded_week_df = encoded_week_df[X_train.columns]\n",
    "\n",
    "    # Run batch inference using registered model\n",
    "    week_preds = model_version.run(encoded_week_df, function_name=\"predict\")\n",
    "\n",
    "    # Get predicted scores (class 1 probabilities)\n",
    "    week_scores = model_version.run(encoded_week_df, function_name=\"predict_proba\")\n",
    "    predicted_scores = pd.Series(np.array(week_scores)[:, 1], name=\"PREDICTED_SCORE\")\n",
    "\n",
    "    # Create prediction column\n",
    "    pred_series = pd.Series(np.squeeze(week_preds), name=\"PREDICTED_RESPONSE\")\n",
    "\n",
    "    # Add prediction columns to the original raw input data\n",
    "    raw_week_df[\"WEEK\"] = week\n",
    "    raw_week_df[\"PREDICTED_RESPONSE\"] = pred_series\n",
    "    raw_week_df[\"PREDICTED_SCORE\"] = predicted_scores\n",
    "\n",
    "    # Convert to Snowpark and append to the unified results table\n",
    "    result_sp_df = session.create_dataframe(raw_week_df)\n",
    "    result_sp_df.write.mode(\"append\").save_as_table(\"PREDICTIONS_WITH_GROUND_TRUTH\")\n",
    "\n",
    "    print(f\"‚úÖ Week {week} scoring complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8503e417-6370-450f-b4b1-81237965498b",
   "metadata": {},
   "source": [
    "# Step 14: Evaluation Test # 51 Sagemaker Model Batch Scoring Completed (SEAI51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67994303-daed-4420-8d90-15a2e7ae7930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DORA Validation: Week 5 predictions were stored\n",
    "query = \"\"\"\n",
    "SELECT util_db.public.se_grader(\n",
    "    'SEAI51',\n",
    "    (actual >= 1),\n",
    "    actual,\n",
    "    5,\n",
    "    '‚úÖ Inference for Week 5 was completed and stored in Snowflake!'\n",
    ") AS graded_results\n",
    "FROM (\n",
    "    SELECT COUNT(*) AS actual\n",
    "    FROM AICOLLEGE.PUBLIC.PREDICTIONS_WITH_GROUND_TRUTH\n",
    "    WHERE WEEK = 5\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Run the query\n",
    "session.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b021e0c-047a-4508-bd6b-81daad341053",
   "metadata": {},
   "source": [
    "# Step 15: Full Table Scoring (All Weeks)\n",
    "To support end-to-end workflows like **ML Observability** and **model retraining**, we now run batch inference over the **entire mortgage application dataset** in one go ‚Äî scoring every row across all weeks.\n",
    "\n",
    "This mirrors common MLOps use cases like:\n",
    "\n",
    "- Rebuilding a model after new data arrives\n",
    "- Comparing historical model performance across time\n",
    "- Feeding labeled prediction data into Snowflake‚Äôs `MODEL_MONITOR` or `MODEL_REGISTRY`\n",
    "- Creating performance dashboards using a single predictions table\n",
    "\n",
    "**Tip**: This is a common pattern in production pipelines, where you may have a nightly or weekly job that scores all open cases or the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15703827-4ca8-43b2-804d-7c30a69f426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 15: Full Table Scoring (All Weeks)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "# Load entire inference dataset from Snowflake\n",
    "raw_all_df = session.table(\"AICOLLEGE.PUBLIC.INFERENCEMORTGAGEDATA\").to_pandas()\n",
    "\n",
    "# One-hot encode categorical columns using same logic as training\n",
    "encoded_all_df = pd.get_dummies(raw_all_df, columns=categorical_cols)\n",
    "\n",
    "# Ensure feature alignment (add missing columns and reorder)\n",
    "for col in X_train.columns:\n",
    "    if col not in encoded_all_df.columns:\n",
    "        encoded_all_df[col] = 0  # Fill missing with 0\n",
    "encoded_all_df = encoded_all_df[X_train.columns]  # Reorder to match training input\n",
    "\n",
    "# Load registered model\n",
    "registry = Registry(session=session, database_name=\"AICOLLEGE\", schema_name=\"PUBLIC\")\n",
    "model = registry.get_model(\"XXX\")   # <-- Match your registered model name\n",
    "model_version = model.version(\"XXX\")  # <-- Match your version model name\n",
    "\n",
    "# Run prediction and probability\n",
    "predictions = model_version.run(encoded_all_df, function_name=\"predict\")\n",
    "proba_predictions = model_version.run(encoded_all_df, function_name=\"predict_proba\")\n",
    "\n",
    "# Attach predictions to original data\n",
    "raw_all_df[\"XXX\"] = pd.Series(np.squeeze(predictions))  # <-- Provide model prediction response name\n",
    "raw_all_df[\"XXX\"] = pd.Series(np.array(proba_predictions)[:, 1])  # <-- Provide model prediction score name\n",
    "\n",
    "# Save the fully scored dataset to Snowflake\n",
    "full_results = session.create_dataframe(raw_all_df)\n",
    "full_results.write.mode(\"overwrite\").save_as_table(\"ALL_PREDICTIONS_WITH_GROUND_TRUTH\")\n",
    "\n",
    "# Preview\n",
    "full_results.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
